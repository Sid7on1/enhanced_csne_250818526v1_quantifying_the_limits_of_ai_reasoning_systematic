{
  "agent_id": "coder1",
  "task_id": "task_6",
  "files": [
    {
      "name": "setup.py",
      "purpose": "Package installation setup",
      "priority": "low"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.NE_2508.18526v1_Quantifying_The_Limits_of_AI_Reasoning_Systematic",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on cs.NE_2508.18526v1_Quantifying-The-Limits-of-AI-Reasoning-Systematic with content analysis. Detected project type: computer vision (confidence score: 8 matches).",
    "key_algorithms": [
      "Ceiling",
      "Resulting",
      "Machine",
      "Prompt",
      "Floor",
      "Representation",
      "Manifold",
      "Operator",
      "Learning",
      "Optimal"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "\n--- chunk_1.txt ---\nPDF: cs.NE_2508.18526v1_Quantifying-The-Limits-of-AI-Reasoning-Systematic.pdf\nChunk: 1/2\n==================================================\n\n--- Page 1 ---\nSystematic Neural Network Representations of Algorithms\nQuantifying The Limits of AI Reasoning:\nSystematic Neural Network Representations of Algorithms\nAnastasis Kratsios kratsioa@mcmaster.ca\nDepartment of Mathematics and Statistics\nMcMaster University and Vector Institute\nOntario, Canada\nDennis Y. Zvigelsky yankovsd@mcmaster.ca\nDepartment of Mathematics and Statistics\nMcMaster University and Vector Institute\nHamilton ON, Canada\nBradd Hart hartb@mcmaster.ca\nDepartment of Mathematics and Statistics\nMcMaster University\nHamilton, ON, Canada\nAbstract\nA main open question in contemporary AI research is quantifying the forms of reasoning neural\nnetworks can perform when perfectly trained. This paper answers this by interpreting reasoning\ntasks as circuit emulation, where the gates define the type of reasoning; e.g. Boolean gates for pred-\nicate logic, tropical circuits for dynamic programming, arithmetic and analytic gates for symbolic\nmathematical representation, and hybrids thereof for deeper reasoning; e.g. higher-order logic.\nWe present a systematic meta-algorithm that converts essentially any circuit into a feedforward\nneural network (NN) with ReLU activations by iteratively replacing each gate with a canonical\nReLU MLP emulator. We show that, on any digital computer, our construction emulates the cir-\ncuit exactly\u2014no approximation, no rounding, modular overflow included\u2014demonstrating that no\nreasoning task lies beyond the reach of neural networks. The number of neurons in the resulting\nnetwork (parametric complexity) scales with the circuit\u2019s complexity, and the network\u2019s computa-\ntional graph (structure) mirrors that of the emulated circuit. This formalizes the folklore that NNs\nnetworks trade algorithmic run-time (circuit runtime) for space complexity (number of neurons).\nWe derive a range of applications of our main result, from emulating shortest-path algorithms\non graphs with cubic-size NNs, to simulating stopped Turing machines with roughly quadratically-\nlarge NNs, and even the emulation of randomized Boolean circuits. Lastly, we demonstrate that\nour result is strictly more powerful than a classical universal approximation theorem: any universal\nfunction approximator can be encoded as a circuit and directly emulated by a NN.\nKeywords: AI Reasoning, Neural networks, Circuit Complexity, Constructive Approximation.\nMSC (2020): 68T07, 68Q17, 68Q05, 68W40, 68N99\n1. Introduction\nSince their mainstream adoption reasoning-based LLM models have redefined the state-of-the-art\nin nearly every computational discipline, from medicine and biology Luo et al. (2022); Zhang et al.\n1arXiv:2508.18526v1  [cs.LG]  25 Aug 2025\n\n--- Page 2 ---\nKratsios, Zvigelsky, and Hart\n(2023), chemistry Bran et al. (2023), finance Yang et al. (2023a,b); Fan et al. (2024); Saqur et al.\n(2025), physics Xu et al. (2025); Qiu et al. (2025); Zhang et al. (2025), epidemiology Chen and\nStadler (2023); Li et al. (2024a), and many others. The success of deep learning models is typi-\ncally understood as a synergy between the expressive potential , the statistical properties, and their\namenability to (gradient-descent-type) optimization algorithms. However, the relationship between\nthese three factors and AI-based reasoning remains somewhat opaque. We focus on understanding\nAI reasoning through the lens of the expressive potential of AIs, by which we mean what such\nsystems are capable of given unlimited amounts of perfect data and idealized training procedures.\nThe AI theory community\u2019s interpretation of \u201cexpressivity potential\u201d of a deep learning model\nis heavily influenced by the approximation-theoretic tradition, shaped by the historical impact of\ntheuniversal approximation theorem of Hornik et al. (1989); Cybenko (1989); Funahashi (1989).\nThis tradition of approximation theorems framed our understanding of neural network expressivity\nas the ability to express (effectively) any function asymptotically, e.g. Yarotsky (2018); Petersen\nand Voigtlaender (2018); Schwab and Zech (2019); Elbr\u00a8 achter et al. (2021); Kratsios and Zamanlooy\n(2022); Kratsios et al. (2022); Gribonval et al. (2022); Schwab and Zech (2023); Voigtlaender (2023);\nSiegel and Xu (2024); van Nuland (2024); Neufeld and Schmocker (2024); Zhang et al. (2024); Ad-\ncock et al. (2025), and various well-behaved functions using relatively few parameters, e.g. Barron\n(1993); Suzuki (2019); Beck et al. (2024); Lu et al. (2021a); G\u00a8 uhring and Raslan (2021); Hutzen-\nthaler et al. (2023); Yang and Lu (2024); Abdeljawad and Dittrich (2024). This lens ultimately\nculminated in extremal function approximation results in two bifurcating directions either: 1) iden-\ntifying irregular activation functions that let fixed-size neural networks approximate essentially any\nfunction with arbitrary precision Yarotsky (2021); Zhang et al. (2022) or 2) constructing networks\nbalancing regularity and approximation power Hong and Kratsios (2024); Riegler et al. (2024).\nThough the approximation-theoretic viewpoint of AI expressive potential has been extremely\nnatural in applications of deep learning in the computational sciences Hutzenthaler et al. (2020a);\nCai et al. (2021); Marcati et al. (2023); Gonon (2023), natural Veli\u02c7 ckovi\u00b4 c (2023), economic Buehler\net al. (2019); Hutzenthaler et al. (2020b) sciences, signal processing Gama et al. (2018); Ma et al.\n(2021); Hovart et al. (2023), etc.; the traditional approximation-theoretic lens fails to quantify, let\nalone answer, one of the central open questions in the contemporary AI landscape. Namely:\nQuestion 1\nWhat problems can feedforward neural networks reason through?\nThe reasoning capabilities of deep learning models has swiftly come into sharp focus in the LLM\ncommunity with most modern LLMs routinely being used, or designed Li et al. (2019, 2020); Lu\net al. (2023), for prompt-based reasoning expressivity is progressively being re-framed in a more\nclassical AI sense as the ability of an AI to reason through mathematical problem solving Didolkar\net al. (2024); Ahn et al. (2024) deductive logic Riegel et al. (2020); Zhang and B\u00a8 olcskei (2024),\ninductive reasoning Cropper et al. (2022), counterfactual thought Wu et al. (2023), multi-modal Li\net al. (2023a), language-based deductive reasoning Hsu et al. (2021); Chen et al. (2022), code-\ngeneration Cohn et al. (2010); Raffel et al. (2020), and a growing wealth of modalities and use-cases.\nThe disconnect between approximation theory and AI-reasoning communities raises a hope for\nanswering (1) by re-examining our approximation-theoretic tools, and the quantification of \u201cstruc-\nture\u201d of a function; guided by the lens of the AI-reasoning community. The main purpose of this pa-\nper is to take a stride forward towards answering (1), by probing the relationship between algorithmic\n2\n\n--- Page 3 ---\nSystematic Neural Network Representations of Algorithms\nreasoning , well-structured functions, and the approximation theory of deep neural networks. Our\nperspective diverges from the smoothness-based viewpoint influenced by classical constructive ap-\nproximation Wojtaszczyk (1997); Lorentz et al. (1996b); Cohen (2003) rooted in its Besov-theoretic\norigins Besov (1959); Triebel (2008), by adopting a new algorithmic complexity -centric perspective.\nThis gap is rooted in the fact that approximation theory oversimplifies functions by viewing\nthem simply as input\u2013output maps; overlooking the fact that functions are algorithmically definable\nrelative to a language, in the sense of mathematical logic Marker (2002), where computation is\ndefined by the compositional execution of elementary computations. From the logical lens, the\ncomplexity of a function is quantified by the number of these elementary computations, called gates,\nrequired to compute it. These gates belong to a pre-specified dictionary of algebraic, logical, and\nanalytic operations, which can be understood as elementary reasoning steps and the directed acyclic\ngraph tracing the composition of the elementary computations executed by these gates formalizes\nchain-of-thought /sequential reasoning conceptualized in the recent LLM literature Wei et al. (2022);\nNye et al. (2022). Consequently, we understand a neural network architecture being able to efficiently\napproximate a function only if it can efficiently encode the underlying algorithm computing it; that\nis, only if the network can correctly emulate each step (or gate) in the sequential reasoning, or\nchain-of-thought, in the algorithm computing/defining the function being approximated.\n  \nor  \n\u03c9-Uniformly Continuous\n(a) Classical approximation theory\n (b) Ours: approximation theory + circuit complexity\nFigure 1: Comparison: Classical approximation theory vs. our complexity-theoretic refinement.\nLeft: Classical approximation perspective views functions fas black-boxes which, when queries\nwith an input xPr0,1sd, yield some output fpxqPRD. All that is assumed of the black-box is a\npriori knowledge of its regularity; e.g. smoothness or uniform continuity, and the best one can hope\nfor is interpolation or approximation (since no internal structure is known of f).\nRight: Weaugment the classical approximation-theoretic perspective with a circuit complexity\nlens by incorporating information on the computation burden required to realize f. This creates\ntransparency, reducing black-box settings to a white-box access of the algorithm computing f. The\nelementary reasoning steps are each gate in the circuit computing f, the chain-of-thought, is the\nend-to-end sequence of computations, and reasoning emulation is not only interpolation of fbut\nalso exact emulation of each step in the computational flow.\nTo best convey our frame of reference, let us briefly examine the common proof strategy employed\nwhen constructing every approximation guarantee that the authors are aware of, from generalist\nuniversal approximation theorems to specialist efficient approximation results. Every approxima-\ntion theorem of which the authors are aware operates in two broad stages. First, they construct an\noptimal function that computes a solution to the given problem of interest; then, a neural network\nis constructed that can (approximately) implement that function. Examples range from worst-case\n(universal) function approximators Yarotsky (2018) which encode wavelets, optimal/robust/regular\n3\n\n--- Page 4 ---\nKratsios, Zvigelsky, and Hart\ninterpolators Vardi et al. (2021); Egosi et al. (2025); Hong and Kratsios (2024) e.g. which encode\ntent functions on the Kuhn triangulation, effective sparse deep learning procedures which emu-\nlate compressed sensing algorithms Adcock et al. (2024); Franco and Brugiapaglia (2024), neural\noperator-based PDE solvers achieving fast convergence rates Marcati and Schwab (2023); Furuya\nand Kratsios (2024); Furuya et al. (2024); Kratsios et al. (2025a) which emulate fixed-point itera-\ntions or compute rapidly converging series, etc. In each case, the constructed neural networks solve\nproblems that are algorithmically computable and the complexity of the neural network required to\nsolve a given problem reflects the complexity of the best possible algorithm for solving that problem.\nBy understanding reasoning in this way, this suggests the following quantitative reformulation\nof Question (1) which also proposes an interpretation of \u201cexpressivity of neural networks\u201d which\nmore closely expresses the popular understanding of artificial intelligence-based reasoning\nQuestion 2\nIf a function is computable, how large must a neural network be to compute it?\nThe idea underpinning our solution to (1) is summarized graphically in Figure 2. Broadly\nFigure 2: Summary of Main Result: Given any (approximate) representation of a computable\nfunction as a circuit, we replace each computational node with an elementary ReLU neural net-\nwork\u2014selected from a prespecified dictionary\u2014that emulates the corresponding computation.\nspeaking, our main result (Theorem 1) shows that any computable function can be approximated\nby a circuit\u2014a directed acyclic graph where each internal node performs an elementary computation.\nThus, by constructing a dictionary of \u201celementary neural networks\u201d that implement each permissible\ncomputation, any computable function can be realized by replacing each node in the circuit with\nits neural counterpart. Moreover, the resulting network mirrors the circuit it computes since:\n4\n\n--- Page 5 ---\nSystematic Neural Network Representations of Algorithms\nTime-to-Space Complexity: The NN has\u00abthe same number of neurons (space complexity ) as\nthe number of gates in the circuit ( time complexity ),\nGraph Structure Preservation: The NN\u2019s computational graph has roughly the same shape as\nthe circuit it computes.\nOur main result, covering cases where an explicit algorithm cannot be elicited in Step 1 above,\nby our universal representation theorem (Theorem 2), which guarantees that any gate, and hence\nany circuit, can be exactly implemented by a large generic neural network on a digital computer.\nHow to UseOur Main Result: Tailored NN Approximation Guarantees Our main result\n(Theorem 1) can be viewed as a unifying meta-theorem that subsumes existing approximation-\ntheoretic results and serves as a general blueprint for deriving new ones. In particular, it yields\nthe following pipeline for producing custom approximation theorems, tailored to a specific setting,\nwhen working on a digital computer:\nStep 1: Specify a circuit efficiently computing the target function.\nStep 2: Represent the circuit as a neural network via Table 1, thereby invoking Theorem 1.\nWe demonstrate this across a range of applications, showing how our main result directly yields\nrepresentation guarantees for randomized Boolean circuits 1, dynamic programming problems on\ngraphs 4, and recursive computation via stopped Turing machine emulation 3.\nMoreover, our main theorem provides a basic form of neural network interpretability : the con-\nstructed model computes a traceable sequence of operations\u2014a \u201cchain of thought\u201d\u2014that imple-\nments a concrete circuit performing a well-defined task.\nTheorem 1 is More Powerful than Universal Approximation For scenarios where one\ndoes not wish to work under the digital computing assumption, e.g., allowing real-valued inputs\nand outputs as in classical approximation theory, Lemma 1 can be applied directly to obtain an\napproximate analogue of the corresponding result. Thus, our main theorem implies theuniversality\nof neural networks for real-valued inputs and outputs (Corollary 5).\nRemark 1 (Why Feedforward Neural Networks with ReLU Activation Functions?) We\nneed only consider feedforward neural networks, not necessarily MLPs, with the ReLU activation\nfunction of Fukushima (1969). This is because there are procedures for converting such into other\nstandard deep learning models, e.g., convolutional networks Petersen and Voigtlaender (2020),\nmulti-head transformers Kratsios and Furuya (2025), and spiking neural networks Singh et al.\n(2023). Similarly, networks using any standard activation function are also known to be approxi-\nmately representable as ReLU networks with similar size and structure; see e.g. Zhang et al. (2024).\n1.1 Related Literature\nNeural Network and Algorithmic Complexity Theory The computational implications of\nneural networks have a long and active history. Early work investigated the Turing completeness\nof recursive deep learning models, particularly recurrent neural networks Siegelmann (2012); P\u00b4 erez\net al. (2019); Chung and Siegelmann (2021); Bournez et al. (2025), and their ability to emulate push-\ndown automata Zeng et al. (1993). More recently, the computational power of modern architectures\nsuch as autoregressive transformers has been studied in Schuurmans et al. (2024).\n5\n\n--- Page 6 ---\nKratsios, Zvigelsky, and Hart\nBeyond recursive models, the computational capabilities of non-recursive architectures have\nalso been explored through the lens of Boolean circuit complexity. For instance, it has been\nshown that there exist (possibly large) transformers capable of encoding finite circuits composed of\nAND ,OR,NOT gates, corresponding to the AC0complexity class Li et al. (2024b). The class TC0,\nwhich additionally allows for majority gates, was studied in Chen et al. (2024); Chiang (2025).\nThere is also limited work exploring the interplay between circuits and feedforward networks Par-\nberry (1994); Vollmer (1999); Karpinski and Macintyre (1997); however, this line of research focuses\non learning-theoretic rather than complexity-theoretic questions. Perspectives from algebraic cir-\ncuit complexity have also been considered, notably in Wang et al. (2024). We also mention the\ngrowing literature showing the computational intractability of perfectly optimized neural networks\nfrom the complexity theoretic vantage point; e.g. Froese et al. (2022); Boob et al. (2022); Bresson\net al. (2020); Bournez et al. (2023); Wurm (2023); Bertschinger et al. (2023); Brand et al. (2023);\nFroese and Hertrich (2023); Ganian et al. (2025), their limits in terms of formal verification Wurm\n(2024), and representing numerical solvers (which are algorithms) neural networks; e.g. certain\nODE solvers are residual-type neural networks Larsson et al. (2017); Zhang et al. (2017) for ODEs,\ncertain integrators Bresson et al. (2020)as neural networks, and certain fixed-point algorithms in\nconvex optimization Kratsios et al. (2025b).\nAI Reasoning in Mathematical Logic More recently, the logical perspective has proven fruitful\nin recasting \u201cchain-of-thought\u201d reasoning as questions in circuit complexity. While a direct connec-\ntion to AI reasoning remains nascent, early deep learning literature briefly explored links between\ndefinable formulae, circuit complexity theory, and VC-dimension Blumer et al. (1989); Karpinski\nand Macintyre (1997). These ideas were revisited in a more informal fashion in Monta\u02dc na and Pardo\n(2009), which considered sample complexity for formula classes, and further developed in Yarotsky\n(2018), which studied the function approximation capabilities of neural networks. The connections\nbetween specific graph-based algorithm computation neural reasoning has also begun to see signifi-\ncant recent investigation, e.g. in Frydenlund (2025); Sanford et al. (2024a,b). There have been some\npreliminary links between computational structure and learnability in Mhaskar and Poggio (2016);\nSchmidt-Hieber (2020); Kohler and Langer (2021) before any connection to computability as well\nas some work de Luca et al. (2025) with links to learning thoery.\nImplications of Machine-Precision and Quantization On Deep Learning Our focus on\ninputs and outputs with a fixed maximal bit-complexity aligns with the growing literature on the\nbenefits of deep learning on digital computers Merrill and Sabharwal (2023); Liu et al. (2023);\nKratsios et al. (2024) as well as the limitations thereof Boche et al. (2023, 2025). This also\navoids purely mathematical pathologies such as the Kolmogorov-Arnol\u2019d representation Kolmogorov\n(1961); Arnold (1963), which as emphasized in the proof of Kahane (1975), fundamentally hinges\non properties of irrational numbers. We allow our networks access to one order more precision than\nthe data they are processing, which reflects the recent uncovering of the power of quantization in\namplifying an AI\u2019s predictive performance Hubara et al. (2018); Jegelka (2022); Jacob et al. (2018).\nThe Expressive Potential Graph Neural Networks via Graph Isomorphisms Tests Pos-\nsibly the most widely accepted exception to understanding of neural network expressive potential\narises in the geometric deep learning community when formulating the expressive potential of graph\nneural networks (GNNs). For that community, a GNN architecture is understood as being expres-\nsive if they can distinguish various families of graphs essentially by computing the Weisfeiler-Leman\ngraph isomorphism tests introduced in Leman and Weisfeiler (1968); see Xu et al. (2019); Beddar-\n6\n\n--- Page 7 ---\nSystematic Neural Network Representations of Algorithms\nWiesing et al. (2024), among many others Sato (2020). There are, nevertheless, a small number of\nworks which frame GNN expressive power in the classical function approximation lens D\u2019Inverno\net al. (2024) as well as a few earlier works focusing on their CNN predecessors Petersen and Voigt-\nlaender (2020); Yarotsky (2022) (which are effectively GNNs on lattices instead of on general graphs).\n2. Preliminaries\nThis section contains all the necessary background and aggregates all notation in order to treat our\nmain results.\n2.1 Digital Computing\nCalculations on digital computers are usually performed in binary floating-point arithmetic. Here,\narithmetic operations on real numbers are actually constrained to a discrete grid of the form\nRqdef.\u201c\"\np\u00b41q\u03b2q`1q\u00ff\nj\u201c\u00b4q\u03b2j\n2j:p\u03b2jqq`1\nj\u201c\u00b4qPt0,1u2q`2*\n. (1)\nIn this (radix-2) representation, the parameter qPNis referred to as the \u201cprecision\u201d and the\nintegers qis the maximum exponent. For convenience, we define the maximal representable value\nin our number system Mdef.\u201c2q`1\u00b42\u00b4q.\nThe set Rqis not closed under the standard addition and multiplication of R. This necessitates\noverflow handling methods, i.e., definitions of the sum or product of two elements in Rqwhen the\nstandard operations produce values outside that set. Throughout, we adopt a modular1conven-\ntion which is standard in digital hardware Patterson and Hennessy (2020), cryptography Menezes\net al. (1996); Shoup (2009), hashing Cormen et al. (2009); Rabin (1981), random number genera-\ntion Knuth (1997); L\u2019Ecuyer (2012), checksums in network protocols Braden et al. (1988); Stallings\n(2020), and low-level programming languages; e.g. C/C++. In modular arithmetic\nx`Rqydef.\u201c p\u00b4 1q\u03b1q`1\u03a3q\nj\u201c\u00b4q`\n\u03b1j2j\u02d8\n`p\u00b4 1q\u03b2q`1\u03a3q\nj\u201c\u00b4q`\n\u03b2j2j\u02d8\npmod 2q`1q\nx\u02c6Rqydef.\u201c p\u00b4 1q\u03b1q`1`\u03b2q`1\u03a3q\nj\u201c\u00b4q`\n\u03b1j2j\u02d8\n\u02c6\u03a3q\nj\u201c\u00b4q`\n\u03b2j2j\u02d8\npmod 2q`1q(2)\nfor any x\u201c\u03a3q\nj\u201c\u00b4q\u03b1j2jandy\u201c\u03a3q\nj\u201c\u00b4q\u03b2j2jPRq. The vectorized versions are defined component-\nwise; akin to their real-valued (infinite-precision) counterparts.\nWe will show that, these modular operations can be exactly computed by ReLU MLPs, in\ncontrast to their non-modular counterparts, which admit only approximate representations; via\nthe standard approximation (Yarotsky, 2017, Proposition 3) via the sawtooth function Telgarsky\n(2015). We also note that modular addition less straightforward to implemented than standard\naddition without overflow by MLPs; which only requires width 2 and depth 1.\n2.1.1 Real-Valued Functions on Digital Computers\nKey to our analysis is the idea of being indistinguishable up to machine precision, as formalized by\nthe following equivalence relation.\n1. Alternative approaches include \u201csaturating\u201d addition/multiplication, e.g. in the Rust language Klabnik and Nichols\n(2023). These alternatives need not yield standard algebraic structures such as rings.\n7\n\n--- Page 8 ---\nKratsios, Zvigelsky, and Hart\nDefinition 1 (Indistinguishably) Letf, g:Rd\u00d1R. We say that fandgare indistinguishable\nat a precision level pfor the base b, or simply write f\u201eg, if: for all xPRd\nq\nfpxq\u201cgpxq.\nRemark 2 Let\u00b5be the uniform probability measure on Rd\nq. Then, for measurable f, g:Rd\u00d1R,\nf\u201egif and only if fandgbelong to the same equivalence class in L8\n\u00b5pRdq.\nRounding Down Onto the Grid When working with real-valued functions in Section 4.2.1,\nboth inputs and outputs must be \u201cplaced on the grid\u201d Rq. For each q, dPN`, we fix a choice of a\nrounding scheme onRdto precision 2q; by which we mean any fixed choice of a metric projection\n\u03c0d:Rd\u00d1Rd\nqsatisfying\n}\u03c0d\nqpxq}8\u201cmin\nzPRdq}z\u00b4x}8 (3)\nfor all xPRd; the choice of the \u21138metric is inconsequential but convenient2.\nExample 1 IfD\u201c1we may consider the rounding scheme \u03c01\nq:R\u00d1Rqfor each xPRby\n\u03c01\nqpxqdef.\u201csignpxqsuptaPRq: 0\u010fa\u010f|x|u. (4)\nThis rounding scheme allows us to work with mathematical\u201d functions f:Rd\u00d1RD\u2014defined\nbeyond our number system Rq\u2014by projecting\u201d them down to a rounded function \u00aff:Rd\nq\u00d1RD\nq.\nConcretely, \u00affis obtained by restricting the domain of fto the grid Rd\nqand then projecting its outputs\ntoRD\nq, whenever they do not lie thereon, using our chosen rounding operation satisfying (3). Once\nwe fix \u03c0D\nq, the map \u00affis defined for each xPRdas\n\u00affdef.\u201c\u03c0D\nq\u02ddf|Rdq. (5)\nIffpRd\nqq \u010eRD\nqthen \u00aff\u201cf; meaning that (extensions of) functions fdefine between the digital\ncomputing grids Rd\nqandRD\nqare preserved under the rounding operation; f\u00de\u00d1\u00aff. We highlight\nthat, while a general fmay not be emulatable by a ReLU network, while it may be possible for its\nrounded version \u00aff. Theorem 2 confirm this, and Theorem 1 shows that the network may be small.\n2.2 Circuits\nWe now formalize G-circuits. We begin by formalizing the computational graph underlying any\ncircuit, and any feedforward neural network.\n2.2.1 Directed Acyclic Graphs (DAGs)\nIn what follows, we consider connected directed acyclic graphs (DAGs). A DAG is a pair of D\u201c\npV, Eqof a (possibly infinite) non-empty set V\u010eNand directed edges E\u0102tpv1, v2qPV2:v1\u2030v2u\nwith the property that Ehas no cycles; i.e. there is no finite sequence pvnqN\nn\u201c1\u010eVsatisfying\npvn, vn`1qPEfor each n\u201c1, . . . , N\u00b41 and such that v1\u201cvN; such finite sequences are called\npaths . A parent of a vertex vPVis a vertex wPDfor whichpw, vqPE, in this case, we say that\n2. This is similar to some modern pathwise approaches to rough analysis, where one fixes a choice of dynamic\nrefinements on which rough integrals are defined, e.g. Cont and Das (2023); Allan et al. (2024).\n8\n\n--- Page 9 ---\nSystematic Neural Network Representations of Algorithms\nvis the child ofw. The set of parents of a vertex vis denoted by pavdef.\u201c twPE:pw, vqPEuand\nthe set of children of vis denoted by ch vdef.\u201c twPE:pv, wqPEu.\nFollowing standard circuit complexity theory, we will use DAGs to encode the computational\ngraphs of algorithms; as is, for instance, implemented by any contemporary deep learning software\nand many standard algorithms.\nA vertex vPVis called an input node if it has no parents; these represent an input (or a\ncomponent of a vector of inputs) of an algorithm whose computational pipeline/graph is encoded\nbyD. A vertex gPDis called an output node if it has no children; output nodes represent points\nan algorithm produces an output (or a component of a vector of outputs). The set of input nodes\nand output nodes are thus, respectively\nInpDqdef.\u201c tvPV: pav\u201cHu and OutpDqdef.\u201c tvPV: ch v\u201cHu\nAll other vertices in D, i.e. those which are neither input nor output nodes, are called compu-\ntation nodes ; the set of all computational nodes is\nComppDqdef.\u201c tvPD: pav\u2030H and ch v\u2030Hu .\nComputational nodes are precisely those where an algorithm implements some computation (to be\nformalized shortly).\nLetd, DPN`. We use DAG to denote the set of connected DAGs with exactly dinput nodes,\nexactly Doutput nodes, and with finitely many computational nodes # Comp pDq\u01038 . These will\nbe used to encode circuits computing functions from RdtoRD.\n2.2.2 Circuits\nWe study neural networks as non-uniform models of computation by showing that these networks\ncan approximately implement infinite (non-standard) circuits and a broad range of finite (standard)\ncircuits. Our analysis includes all finite Boolean Jukna (2012b), arithmetic Lloris Ruiz et al. (2014,\n2021), probabilistic Boolean, and even certain Pfaffian circuits Karpinski and Macintyre (1997);\nMorton and Turner (2015).\nLetGbe a subset of\u01648\nn\u201c1rRn:Rscontaining the identity on R, denoted by 1 R. The elements\nofGare call G-gates , or simply gates when Gis contextually apparent; where rRn:Rsdenotes\nthe set of all functions from RntoR. An elementary G-circuit Ais a triple Adef.\u201c pV, E,Gqof an\nelementary DAG D\u201cpV, Eqin DAGd,D, with V\u010eN, and a set Gdef.\u201c tgvuvPVsuch that for each\ninput and output node vPInpDqYOutpDqand\ngvpxq\u201cx\nfor each xPR, and satisfying the compatibility condition for each computational node vPComppDq\nIm`\ngv1, . . . , g vn\u02d8\n\u010edompgvq (Comp)\nwheretv1\u0103\u00a8\u00a8\u00a8\u0103 vnu\u201cpavare the set of parent nodes feeding into v. An elementary G-circuit A\ncomputes a function Cpt pAq:Rd\u00d1RDdefined recursively for each xPRdby:\nLet InpDq\u201cpviqd\ni\u201c1, with v1\u0103\u00a8\u00a8\u00a8\u0103 vd, define each xpviqPRby\npxpv1q, . . . , xpvdqqdef.\u201c px1, . . . , x dqwhere v1\u0103\u00a8\u00a8\u00a8\u0103 vd3\n3. The ordering makes sense since there are distinct integers by definition of V\u010eN.\n9\n\n--- Page 10 ---\nKratsios, Zvigelsky, and Hart\nfor each wPComppDqorder paw\u201cpw1, . . . , w Dwqbyw1\u0103\u00a8\u00a8\u00a8\u0103 wDw1and define xpwqPRby\nxpwqdef.\u201cgw`\nxpw1q, . . . , xpwDwq\u02d8\nagain using the natural ordering on Out pDq\u201cpuiqD\ni\u201c1given by u1\u0103\u00a8\u00a8\u00a8\u0103 uD, we output\nCptpAqpxqdef.\u201c pxpuiqqD\ni\u201c1.\nIn this way, we distinguish an algorithm and the function it computes, as there may be more\nalgorithmic representations of a single function. This is analogous to the distinction between the\nparametric MLPs and the function they realize made in Petersen and Voigtlaender (2018).\n2.2.3 G-Circuit Surgery\nThe formalization of our main result, as illustrated in Figure 2, relies on the systematic replace-\nment of the computations in a G-circuit with standard ReLU MLPs (possibly leveraging a skip\nconnection). This systematic replacement is formalized by the surgery operation defined between\ntwo \u201ccompatible\u201d G-circuits as at a specific node.\nDefinition 2 (Surgery at a Node) LetA\u201cpVA, EA, GAqandB\u201cpVB, EB, GBqbeG-circuits\nwith OutpBq\u201ct vB\noutu. Let vPComppAqand (if it exists) a surjective \u201crewiring map\u201d f: pav\u00d1\nInpBq. The B-surgery of Aatvviafis aG-circuit, denoted by pf,BqrAs\u201cp \u02dcV ,\u02dcE,\u02dcGq, and defined\nby\n(i)\u02dcVdef.\u201cVA\u0164VBztvu\n(ii) \u02dcEdef.\u201c`\nEAzppav\u02c6chvqlooooomooooon\nDetach v\u02d8\u0164EBloomoon\nAddB\u0164tpu, fpuqq:uPpavu\u010f\ntpvB\nout, u:uPchvulooooooooooooooooooooooooooomooooooooooooooooooooooooooon\nAttach B\u2019s Inputs and Outputs to v\u2019s Children.\n(iii) \u02dcGdef.\u201cGB\u0164GAztgvu.\nWe call the surgery flawless if fis a bijection.\nHaving formalized defining a surgery at a given node, we can now formalize replacing several nodes\nin aG-circuit by several \u201creplacement\u201d G-circuits.\nDefinition 3 (Surgery) LetA\u201cpVA, EA, GAqbe aG-circuit, B\u00a8def.\u201c pBwdef.\u201c pVw, Ew, GwqqvPW\nbe an ordered family of G-circuits with W\u201cpwtqT\nt\u201c1an ordered subset of ComppAqand for each\nwPWletfw: paw\u00d1InpBwqbe a surjective \u201crewiring map\u201d. Then, the B\u00a8-surgery of Aby\nf\u00a8def.\u201c pfwqwPWis the G-circuit, denoted by pf\u00a8,B\u00a8qrAs, and defined recursively as follows\nA0def.\u201cA\nAt`1def.\u201c pfwt,BwtqrAtsfort\u201c1, . . . , T\u00b41\npf\u00a8,B\u00a8qrAsdef.\u201cAT.\nIf, for each t\u201c1, . . . , T ,pfwt,Bwtqis flawless, then we say that pf\u00a8,B\u00a8qrAsis flawless. If, addition-\nally, T\u201c# ComppAqthenpf\u00a8,B\u00a8qrAsis completely flawless.\n10\n\n--- Page 11 ---\nSystematic Neural Network Representations of Algorithms\n2.3 Notation\nWe round off this section by collect all the notation, not already introduced, required to formulate\nour main results and proofs. Formal definitions of every gate used to define our circuits can be\nfound in the paper\u2019s Appendix Section A.4. Any additional notation, not already introduced thus\nfar, and required for the formulation of our main results is not introduced.\nSpecial Matrices In what follows, we will make use of the following \u201cspecial\u201d vectors and ma-\ntrices. For every BPN`, we use 1BPt0,1uBto denote the vector with all entries all equal to 1.\nWe distinguish the B\u02c62Bblock matrix A2,Bdef.\u201c rIB\u02c6B, IB\u02c6Bssince: for every a, bPRdwe have\na`b\u201cA2,Bpa, bqJ.Similarly, we distinguish the B\u02c62Bblock matrix A\u00b4\n2,Bdef.\u201c rIB\u02c6B,\u00b4IB\u02c6Bssince:\nfor every a, bPRdwe have a\u00b4b\u201cA\u00b4\n2,Bpa, bqJ.Let \u03a0 2,Bdef.\u201c\u00a8\n\u02dd0B\u02c6BIB\u02c6B\nIB\u02c6B0B\u02c6B\u02db\n\u201adenote the 2 B\u02c62B\npermutation matrix. For each pa, bqPR2B, we have \u03a0 2,Bpa, bq\u201cpb, aq.\nMisc IfXandYare non-empty sets, we denote the set of all functions from XtoYbyrX:Ys.\nWe now present our two main results.\n3. Main Results\nOur main result, Theorem 1 below, considers the more reasonable cases where the target func-\ntion is itself computable by a G-circuit with any gate listed in Table 1 (and defined formally in\nAppendix E) while implementing those gates in the correct underlying computation graph (DAG)\nstructure directly. Doing so not only emulates the underlying chain-of-thought/reasoning implicit in\nthe target function itself, but it ensures that the approximator has (spatial) complexity scaling with\nthe (time) complexity of the algorithm (circuit) used to compute fitself. Our result thus formalizes\nand validates the folklore that \u201cneural networks trade time complexity for space complexity\u201d.\nTheorem 1 (Universal Reasoning - Via Circuit Emulation) LetA\u201cpE, V,Gqbe aG-circuit,\nwhose gates are listed in Table 1. Then there is a complete flawless surgery pf\u00a8,B\u00a8qdef.\u201c pfv,BvqvPComppAq\nofAwhere, for each vPComppAq,Bvis a ReLU MLP whose depth and width is bounded described\nin Table 1 according to its gate, and pf\u00a8,B\u00a8qis a ReLU`-FFNN.\nAt this stage, Theorem 1, covers most functions one may encounter in practice. However,\nit is natural to wonder if there are any circuits which are not implementable by a ReLU neural\nnetwork. Our following result shows that no such circuit exists as every function mapping from the\nfinite precision spaces Rd\nqtoRD\nqisexactly representable by a ReLU neural network. This result is\nthus not an approximation theorem, which has a positive approximation error, but more akin to a\ndigital computing analogue of the Kolmogorov-Arnold superposition theorem (see e.g. Kolmogorov\n(1956); Arnol\u2019d (1959)). Namely, we show that every function on a digital computer can be exactly\nemulated by a ReLU MLP of double its precision up to machine precision; i.e. up to the \u201erelation.\nHowever, as with most universal approximation guarantees, the number of parameters required for\nthe emulation of the worst-behaved function is wildly infeasible.\nTheorem 2 (Universal Circuit Design By ReLU Networks) Fixd, D, qPN`. There exists\na depth 4, width d23`2dq, and less than d3222`2qdnon-zero parameters \u201cuniversal encoder\u201d ReLU\n11\n\n--- Page 12 ---\nKratsios, Zvigelsky, and Hart\nMLP \u03a6Enc:Rd\u00d1R2dqwith the property that: for each f:Rd\nq\u00d1RD\nqthere exists a D\u02c622dq-matrix\n\u03b2fwith entries in Rqsatisfying\n\u03b2J\nf\u03a6Encpxq\u201cfpxq\nfor all xPRd\nq. Note that \u03a6fdef.\u201c\u03b2J\nf\u03a6Encis a ReLU MLP with the same depth and width as \u03a6Enc\nand less than d3222`qd`2pd`Dqpmore non-zero parameters.\nGate Depth Width No. Param. Reference\nBit Manipulators\nBit Decoder (see (23)) 1 2 q`2 2 q`10 Prop. 1\nBit Encoder (see (22)) 3 rlog2p2M`1qs`9 4pq`1qp32M`17q 404M`256Mq`155q`171 Prop. 2\nLeft Shift LSHIFT 1 B 2B\u00b41 Lem. 20\nRight Shift RSHIFT 1 B 2B\u00b41 Lem. 21\nBitwise Arithmetic Gates\nAddition`n2Bn B tlog2pnqu \u2014 Lem. 26\nMultiplication \u02c62B2B2\u2014 Lem. 31\nIdentity In 1 2 n 4n (Petersen and Zech, 2024, Lemma 5.1)\nModular Arithmetic Gates\nAddition`Rq Opqq Opqq \u2014 Lem. 32\nMultiplication \u02c6Rq Opq2q Opq2q \u2014 Lem. 33\nAnalytic Gates\nConstant x\u00de\u00d1c 1 2 n 4n Lem. 10\nIndicator Half-Line Ira,8q 2 1 5 Lem. 11\nIndicator Compact Interval Ira,bs(a\u0103b) 2 2 9 Lem. 13\nIndicator of Point I\u00a8\u201ca 4 4 n 18n`5 Lem. 14\nCeiling r\u00a8s r log2p21`emax`1qs`4 21`emax`1 25p21`emax`1q Lem. 15\nPredicate Logic on BBits\nEquality\u201d 3 2 B 9B Lem. 2\nNAND 2 B 6B Lem. 3\nNOT 1 B 3B Lem. 4\nAND 1 B 4B Lem. 5\nOR 1 B 6B Lem. 6\nXOR 2 2 B 8B Lem. 7\nImplication 1 B 4B Lem. 8\nPropositional Logic on BBits\np@xPt0,1uBq\u03d5px,\u00a8q\u201c1 5 `B`D B ``\nW`3\u00a82B\u02d8\n13B``\n1`S`22B`5\u02d8\nLem. 9\nTropical Operations\nMaximum of nInputs rlog2pnqs 3n 16n Lem. 22\nMedian of 2 n`1 Inputs 11 n`3 6 n`3 \u2014 Lem. 23\nMajority Vote Between nInputs 11 tn{2u`4 6 tn{2u`3 \u2014 Lem. 24\nTable 1: Reference for ReLU MLP emulator of each gate in Gin Theorem 1, and summary of their\ndepth, width, and the number of non-zero parameters.\nA closer look at the construction in Theorem 2, mirrors the constructions of most optimal\nuniversal approximation theorems for non-smooth functions; e.g. Yarotsky (2018); Shen et al. (2022)\nwithout regularity constraints as in Hong and Kratsios (2024); Riegler et al. (2024), shows that the\nencoder \u03a6 Encplaces a spike at each \u201cgrid\u201d points pinRd\nq, i.e. an indicator function of Ix\u201cpand\nthen scales the value of that indicator according to the target function value fppqat that point. As\nillustrated in Figure 3, this simply constructs the \u201cspiky surface\u201d\u0159\nqPRdqfppqIx\u201cp. This function can\n12\n\n--- Page 13 ---\nSystematic Neural Network Representations of Algorithms\nbe computed by an algorithm, see Appendix D, whose run time is lower-bounded by the number\nof spikes fppqIx\u201cpneeded to be constructed, which is \u2126 ppdqand thus suffers from the curse of\ndimensionality.\nx0.0\n0.2\n0.4\n0.6\n0.8\n1.0y\n0.00.20.40.60.81.0f(x, y)\n0.00.10.20.30.40.50.60.70.8Ground Truth: f(x,y)=x+y\nx0.0\n0.2\n0.4\n0.6\n0.8\n1.0y\n0.00.20.40.60.81.0F(x,y)\n0.00.20.40.60.81.0Algorithmic Contruction of Approximator for f(x,y):=sin(x)ex\nFigure 3: Illustration of the Worst-Case Approximator constructed in Theorem 2.\nTheorem 2 uses a ReLU feedforward neural network to compute a circuit which separately\nassigns a value to every possible input pointing Rd\nq.\nWe now examine several implications of our main result before delving into its proof.\n4. Applications\nWe showcase some of the implications of Theorem 1 before providing the rather lengthy proof of\nour main result.\n4.1 Computability Implications and Circuit Complexity\nWe now focus on the computability theoric implication of our main result, from implications on\nTuring-type computability to randomized and deterministic boolean circuit complexity guarantees.\n4.1.1 Boolean-Computable Functions\nLetf:t0,1uB\u00d1t0,1ube a Boolean function. A quick computation shows that there are exactly 22B\nsuch functions; thus, the number grows double exponentially in the number of input bits, mirroring\nTheorem 2. Our first application of our theory is on the smallness of the representation of any such\nfunction by ReLU MLP. Namely, we bring down the worst-case complexity for computing a Boolean\nfunction by a ReLU FFNN from Op22BqtoOp2Bq!\nCorollary 1 (Generic Boolean Functions are Simple To implement by ReLU MLPs) Let\nBPN`. Then there is an absolute constant c\u01050such that: for every Boolean function f:\n13\n\n--- Page 14 ---\nKratsios, Zvigelsky, and Hart\nt0,1uB\u00d1t0,1uthere exists a ReLU FFNN with at-most\n5\u02c6\n1`c`logpBq`logplogpBqq\nB\u02d9\n2BPOp2Bq.\nnon-zero parameters.\nProof [Proof of Corollary 1] Since fis Boolean, then the main result of Lozhkin (1996), as presented\nin (Jukna, 2012a, Theorem 1.15 and page 26), implies that there exists an absolute constant c\u01051,\nand a Boolean circuit Awith gates in \u02dcGdef.\u201c tAND 2,OR2,NOTuwith\nKdef.\u201c\u02c6\n1`c`logpBq`logplogpBqq\nB\u02d92B\nB(6)\ncomputation units such that\nCptpAq\u201cf (7)\ni.e.Acomputes f. Now, Lemmata 6, 5, and 4 imply that each gate in \u02dcGcan be implemented by a\nReLU MLP using no-more than width B, depth 1, and 5 Bnon-zero parameters implementing any\ngate in \u02dcG. Thus, (7) and (6) implies that fcan be computed by a ReLU MLP circuit with at-most\nK5Bnon-zero parameters.\nWe emphasize that Corollary 1 is nearly optimal nearly matches the lower-complexity bound\nof Muller (1959) for a circuit on tAND 2,OR2,NOTuimplementing a generic Boolean function\nonBbits; which requires at-least \u0398 p2Bqcomputation nodes.\n4.1.2 Computable Randomized Logical Algorithms pRTC0q\nLetGbe a (non-empty) set of Boolean gates and let Abe aG-gate on n`minputs, where n, mPN`.\nLetX\u00a8def.\u201c pXkqn`m\nk\u201cn`1be independent Bernoulli random variables taking value 1 with probability\np\u01051\n2, henceforth defined on a common probability space p\u2126,B,Pq. We say that the associated\np-randomized circuitpA, X\u00a8qcomputes a Boolean function f:t0,1un\u00d1t0,1uwith probability pif:\nfor each xPt0,1un\nP`\nCptpx, X n`1, . . . , X n`mq\u201cfpxq\u02d8\n\u011bp. (8)\nThe class of all such algorithms is typically denoted by RTC0.\nCorollary 2 (Derandomization of Randomized Logical Circuits) LetB, m, k, K, \u2206, wPN`\nwith k\u010fK. Let f:t0,1un\u00d1t0,1ube computable by p-randomized circuit with Boolean gates in\ntAND 2,OR2,NOTuand of size K, depth \u2206, and width 2with each gate having fan-in at-most k.\nThen, there is a ReLU feedforward neural network f:RB\u00d1Rof width 60BK`6and depth\n33BK`3`4\u2206computing f(deterministically).\n4.1.3 Finite-Time Turing Machine Simulation\nTransductors areTuring machines well-suited to tasks requiring continuous streaming of inputs and\noutputs, with its internal logic dictated by the usual Turing machine operations. Formally, a trans-\nductor Mis a specialized Turing machine, defined as a 7-tuple: M\u201cpQ,\u03a3,\u0393, \u03b4, q 0, qaccept, qrejectq\nwhere: Qis the set of states, \u03a3 is the input alphabet, in this case \u03a3 \u201c t0,1uB, \u0393 is the tape\nalphabet, \u03b4:Q\u02c6\u03a3\u02c6\u0393\u00d1Q\u02c6\u0393\u02c6tL, Ru\u02c6\u03a3 is the transition function, q0is the initial state,\nqaccept, qrejectPQare the accepting and rejecting states.\n14\n\n--- Page 15 ---\nSystematic Neural Network Representations of Algorithms\nAt every computational step, Meither reads a symbol to the input tape or writes a symbol to\nthe output tape. At each step tPN`,Mtransitions by reading the current input itP\u03a3 and writing\na corresponding output symbol otP\u0393, and then changing its state according to \u03b4. The machine M\nisoblivious , in the sense that for each step t, the transductor (deterministically) must read an input\nsymbol and write an output symbol. Thus, using the transduction function \u03b4, for each state qPQ\nand input symbol itP\u03a3, the machine reads it, processes it to determine the next state, writes an\noutput symbol, and moves the tape head.\nFormally, for each tPN`, the machine: 1) Mbegins state qt, 2) it reads the symbol itfrom\nthe input tape, 3) it then writes the symbol otto the output tape (as determined by \u03b4), and 4) it\ntransitions to the next state qt`1. The transductor Monly halts when qt\u201cqaccept orqt\u201cqreject\n(similar to a standard Turing machine). Thus, when \u03a3 \u201ct0,1uB, for some BPN`, then: for each\nxdef.\u201cq0P\u03a3 and every tPN`we write\nMpxqtdef.\u201cqt.\nWe thus have the following guarantee that a ReLU neural network can\nCorollary 3 (Finite-Time Turing Machine Simulation with Explicit Chain-of-Thought )\nLetMbe a transductor with input alphabet \u03a3\u201ct0,1uBand output alphabet t0,1u. Then, for every\nreasoning time TPN`, there is a ReLU neural network \u02c6f:RB`1\u00d1Rof depth OpTqand with\nOpTlogpTqq, with Osuppressing the dependence on B, computation nodes computing TonRB\nq\n\u02c6fpx, tq\u201cMpxqt\nfor all xPt0,1uBand each computation during the CoT t\u201c0, . . . , T .\nMoreover, \u02c6falso has depth OpTqandOpT2logpTqqcomputation units.\n4.2 Dynamic Programming\nWe now apply our main result to demonstrate that ReLU FFNNs can solve various pure dynamic\nprogramming (DP) problems. We focus on graph-based issues due to their central role in geometric\ndeep learning. Specifically, we examine neural network computations for the traveling salesperson\nproblem (TSP) and the calculation of minimal graph distances between all pairs of nodes. The\nformer is particularly notable for extremal graphs such as expanders Bollob\u00b4 as (2001), illustrated\nin Figure 4a, while the latter is most relevant for graph approximations of manifolds, a classical\ntechnique in manifold learning McInnes et al. (2018); Tenenbaum et al. (2000); Balasubramanian\nand Schwartz (2002), see Figure 4b.\nOur main result establishes that any positive result in pure dynamic programming (DP)\u2014see,\ne.g., Jukna (2023)\u2014automatically yields an upper bound on the complexity required by a ReLU\nfeedforward network to solve the corresponding pure DP problem. We demonstrate this by showing\nthat the set of all shortest path distances on any given graph can be computed by a single fixed\nReLU network, independent of the specific graph, though dependent on the number of vertices, with\nonly cubic complexity. That is, the network may be large, but it remains computationally feasible.\nWithout loss of generality, we restrict attention to the complete graph, as any other graph can be\nobtained by assigning sufficiently large weights to the edges to be effectively removed. Let kPN`\nand denote the complete graph on Vkdef.\u201c t1, . . . , kubyKk\u201cpVk, Ekqwhere Ekdef.\u201c tt i, ju:i, jP\nVkandi\u0103juwhich we recall has has k\u2039def.\u201ckpk\u00b41q{2 distinct edges. Every possibly assignment\n15\n\n--- Page 16 ---\nKratsios, Zvigelsky, and Hart\n(a) All shortest paths problem - computing the\nshortest weighted distances between all pairs of\nnodes on a graph.\n(b) Graph Geodesic Distance - Important on\nWeights Graph Approximations to Riemannian\nManifold.\nFigure 4: Extremal examples of Graphs in Geometric Deep Learning.\nof edge weights w\u201cpweqePEkPp0,8qk\u2039induces a shortest path distance dwonKkdefined for any\npair of vertices i, jPVkby\ndwpi, jqdef.\u201cmin\n\u03b3\u00ff\neP\u03b3we\nwhere the minimum is taken over all paths \u03b3from itoj; i.e. over all sequences of edges \u03b3\u201c\npe1, . . . , e kqwhere i1\u201ci,jk\u201cj, and for l\u201c1, . . . , k\u00b41 we have el\u201cpil, jlqandjl\u201cil`1. The\nall-pairs shortest paths (ASP) problem to construct a circuit which given any wPp0,8qk\u2039returns\nthe flattened distance matrix\nDwdef.\u201c pdwpi, jqqi,jPEk.\nBy (Jukna, 2023, Example 1.8) there exists a tropical Gdef.\u201c tmin,`2ucircuit AASP withOpk3q\nsolving the ASP problem. That is, for every set of positive edge-weights wPRk\u2039\nqwe have\nComppAASPqpwq\u201cDw. (9)\nMoreover, the complexity of AASPis optimal in the class of such tropical Gdef.\u201c tmin,`2ucircuits\ndue to the marching lower-bound in (Jukna, 2023, Corollary 2.3). Applying Theorem 1 we deduce\nthat\nCorollary 4 (ReLU Networks Solve the ASP problem with Cubic Complexity) For ev-\nerykPN`, there is a ReLU FFNN \u03a6ASP:Rk\u2039\nq\u00d1RwithOpk3qnon-zero parameters such that\n\u03a6ASPpwq\u201cDw\nfor each wPRk\u2039\nq.\n4.2.1 (Optimal) Universal Approximation via G-Computability\nWe say that a function fPrRd:RDsisG-computable if there exists an elementary G-circuit Asuch\nthatfis indistinguishable from Cpt pAqonRd\nq,b; i.e.\nCptpAq\u201ef.\n16\n\n--- Page 17 ---\nSystematic Neural Network Representations of Algorithms\nMany mathematically interesting or even simple functions are not computable in the above sense,\ne.g. the radical function x\u00de\u00d1?xonR. We expand our circuit-based notion of computability to\nallow for some asymptotic structure often formalized by recursion. Instead, we draw inspiration\nfrom constructive approximation theory, see Lorentz et al. (1996a) or its modern deep learning\ncounterpart Elbr\u00a8 achter et al. (2021); Gribonval et al. (2022), which describes functions as being\napproximable if they can be approximately expressed at a given worst-case rate.\nComputable functions on Rdand computable reals are typically thought of as those objects for\nwhich there exists a sequence of (some) model of computability, e.g. circuits or Turing machines,\neach of which produces approximate representations of that number to a strictly higher number\nof decimals of accuracy; see e.g. Weihrauch (2000). Similarly, we understand a function as being\ncomputable in our G-circuit our framework if it can be asymptotically approximated to arbitrary\nprecision by a fixed sequence of machines, e.g. circuits or Turing machines, for all computable inputs.\nThough these ideas capture the idea of algorithmic complexity, they need not reflect the rate at\nwhich functions in a given class can be approximated; or rather, approximately computed .\nRecall that, for each q, dPN`, we fix a choice of a rounding scheme on Rdto precision 2qin (3).\nLet\u03c9:r0,8q\u00d1r 0,8qbe a modulus of continuity, i.e. it is monotone increasing, right-continuous\nat 0, with \u03c9p0q \u201c0, and let \u03c1:r0,8q \u00d1 r 0,8qbe a rate function, by which we mean a (not\nnecessarily strictly) monotonically increasing continuous function.\nDefinition 4 (Computable Function) A function f:Rd\u00d1RDisG-computable if: exists a\nsequence of G-circuitspAqq8\nq\u201c1withAqof size Op\u03c1ppqqsatisfying\nmax\nxPr0,1sd\u203a\u203aCptpAqq\u02dd\u03c0d\nqpxq\u00b4fpxq\u203a\u203a\u00c0\u03c9p2\u00b4pq. (10)\nThe set of all such functions f:Rd\u00d1RDis denoted by G\u03c9\n\u03c1pRd,RDq.\nThis definition is motivated by the elementary inequality that describes the computability of a\nfunction in two parts. 1) The discretization error from considering a problem at a fixed resolution p\nand 2) the computation error arising from the difficulty of algorithmically computing the discretized\nfunction via a G-circuit.\nLemma 1 (Discretization-Computation Decomposition) Letf:Rd\u00d1RDbe any function,\nletqPN`, and define\n\u00affqdef.\u201c\u03c0D\nq\u02ddf\u02dd\u03c0d\nq. (11)\nFor any G-circuit Acomputing CptpAq:Rd\nq\u00d1RD\nq, and any xPr0,1sd\n\u203a\u203afpxq\u00b4CptpAq\u02dd\u03c0d\nqpxq\u203a\u203a\u010f2\u00b4p`1`}fpxq\u00b4f\u02dd\u03c0d\nqpxq}8loooooooooooooooooomoooooooooooooooooon\nDisc Err.: (I)`}\u00affqpxq\u00b4CptpAq\u02dd\u03c0d\nqpxq}8loooooooooooooooomoooooooooooooooon\nCompute Err.: (II)\nAs with most standard notions of computability and approximation spaces, we find that any\ncontinuous function is computable. More interestingly, we find that the only real quantity which\nwe want to control is the computation error in (II).\nCorollary 5 (Reasoning Implies Universal Approximation) Iff:r0,1sd\u00d1RDuniformly\ncontinuous with modulus of continuity \u03c9then, fPG\u03c9\n2pd`DqppRd,RDq.\n17\n\n--- Page 18 ---\nKratsios, Zvigelsky, and Hart\nIn other words, for every qPN`there is a ReLU NN AqwithCptpAqq:Rd\nq\u00d1RD\nqof size Op2pd`Dqpq\nsatisfying\nsup\nxPr0,1sd\u203a\u203afpxq\u00b4CptpAq\u02dd\u03c0d\nqpxq\u203a\u203a\u010f2\u00b4p`1`\u03c9p2\u00b4p`1qloooooooooomoooooooooon\nDisc Err..\nIn particular, (II)is zero.\nUnlike classical uniform approximation theory for MLPs, we see that dyadic regression trees are\ncomputable at a finite resolution.\n5. Conclusion\nWe present a systematic meta-algorithm that converts essentially any circuit into a feedforward\nneural network (NN) with ReLU activations by iteratively replacing each gate with a canonical\nReLU MLP emulator (Theorem 1). This construction exactly emulates the original circuit on\nmodern digital computers, with the network\u2019s size scaling with the circuit\u2019s complexity and its\ncomputational graph mirroring the structure of the emulated computation. In this way, neural\nnetworks trade algorithmic runtime for network size, formalizing a folklore principle of deep learning.\nOur results imply universal approximation (Corollary 5) while going further: they yield new\nguarantees ranging from randomized logic emulation to exact Turing-machine simulation, show-\ning that no reasoning task lies beyond the reach of neural networks. We further obtained a\nsuperposition-type result (Theorem 2) demonstrating that any circuit computing a function from\nRd\nqtoRD\nqcan be implemented by a (possibly very large) neural network.\nLastly, we demonstrate that our result is strictly more powerful than a classical universal ap-\nproximation theorem: any universal function approximator can be encoded as a circuit and directly\nemulated by a NN. By connecting circuit complexity, universal approximation, and black-box func-\ntion access, our work opens a concrete pathway toward a theory of AI reasoning of neural networks.\nFuture Work In the near future, we would like to extend our results to handle non-Euclidean\ninputs and outputs, as in the geometric deep learning literature Bronstein et al. (2021); Kratsios\nand Papon (2022) and infinite-dimensional inputs and outputs, as in the operator learning litera-\nture Chen and Chen (1995); Lu et al. (2021b); Li et al. (2023b).\nOne may interpret best approximations of Besov functions, studied in classical approximation\ntheory Lorentz et al. (1996a); Wojtaszczyk (1997) and compressive sensing Adcock et al. (2022), as\n\u201cconverging\u201d sequences of circuits whose elementary operations are scaling, translation, and summa-\ntion of objects such as wavelets Daubechies (1992), polynomial splines DeVore and Sharpley (1993),\nor sparse polynomials Adcock et al. (2018); respectively. An interesting future research direction\ncould develop a general method for translating these classical constructions into the language of\ncircuit complexity, thus directly translating them into the framework introduced in this paper.\nAcknowledgements\nA. Kratsios acknowledges financial support from an NSERC Discovery Grant No. RGPIN-2023-\n04482 and No. DGECR-2023-00230. They also acknowledge that resources used in preparing this\nresearch were provided, in part, by the Province of Ontario, the Government of Canada through\nCIFAR, and companies sponsoring the Vector Institute4.\n4. https://vectorinstitute.ai/partnerships/current-partners/\n18\n\n--- Page 19 ---\nSystematic Neural Network Representations of Algorithms\nA. Kratsios would like to thank Samuel Lanthaler for the early work together on this manuscript\nand for the very enjoyable discussions. The authors would also like to thank Giovanni Ballarin for\nthe helpful references. They would also like to the members of the \u201cAI Reasoning reading group\u201d,\nlead by FuTe Wong, at the Vector Institute for their valuable feedback and insightful discussions.\nAppendix\nOur paper\u2019s appendix is organized as follows.\nTable of Contents\nA Proof of Theorem 1 19\nA.1G-Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nA.2 Logical Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nA.3 Predicate (First-Order) Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nA.4 Analytic Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\nA.5 Bit Encoder and Decoder Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\nA.6 Tropical Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\nA.7 Binary Arithmetic Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\nA.8 Modular Arithmetic in Rq. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\nB Proof of Theorem 2 42\nC Proof of Applications and Corollaries 42\nD Pseudocode Yielding The Universal Approximator of Theorem 2 44\nE Quoted/Known ReLU MLP Constructions 45\nA. Proof of Theorem 1\nHaving formalized the notion of a surgery, to prove Theorem 1 it is enough to construct a local\nsurgery for each gate in G; as defined precisely in Section A.1.\nA.1 G-Gates\nWe now provide a fully-explicit description of the gates making up G, referred to in Theorem 1.\nAlgebraic Operations The algebraic operations considered here are enough to express the\nstandard R-algebra structure on Rd\nq, to overflow handling by \u03c0. These consist of the classes\nGalgdef.\u201c t`n,\u00b4n,\u02c6n,\u00a8\u00b41:n, In:nPN`uwhere, for each nPN`, we define\n1.Addition:`n:Rn\u02c6Rn\u00d1Rn\nq, sendspx, yq\u00de\u00d1`\n\u03c0pxi`yiq\u02d8n\ni\u201c1,\n19\n\n--- Page 20 ---\nKratsios, Zvigelsky, and Hart\n2.Additive Inverse: \u00b4n:Rn\u00d1Rn\nq, sends x\u00de\u00d1`\n\u03c0p\u00b4xiq\u02d8n\ni\u201c1,\n3.Multiplication: \u02c6n:Rn\u02c6Rn\u00d1Rn\nq, sendspx, yq\u00de\u00d1`\n\u03c0pxiyiq\u02d8n\ni\u201c1,\n4.Identity: idn:Rn\u00d1Rn\nq, sends x\u00de\u00d1`\n\u03c0pxiq\u02d8n\ni\u201c1.\nAnalytic Operations In what follows, we use J(resp. K) to respectively define either porr(resp.\nqors). We consider the elementary analytic operations Ganndef.\u201c texpn,logn:nPN`uYt IJa,cK:\na, cPR, a\u010fcuYt \u00afzm:zPRn\nb,q:m, nPN`uwhere for each nPN`and each pair of real numbers\na\u010fc\n1.Constants: For each zPRm\nq, let \u00afzm:Rm\u00d1Rn\nqsends x\u00de\u00d1z,\n2.Indicator Functions: fora, cPRm\nqwith ai\u010fcifori\u201c1, . . . , m ,IJa,cK:Rn\u00d1t0,1usends\nxPRnto 1 if xPJa, cKand 0 otherwise\nLogical Operations We consider the following class of logical operations; here we follow the\nconvention that TRUE=1andFALSE =0. We begin with the predicate (0thorder logical operations)\nGlgcdef.\u201c t_n,^n,\u2423m,\u011bn,\u010fn,\u201dn:n, mPN`, n\u011b2uwhere for each n, mPN`,with n\u011b2, we have\n1.Conjunction: ^n:t0,1un\u02c6t0,1un\u00d1t0,1unsendspx, yq\u00de\u00d1p mintai, biuqn\ni\u201c1,\n2.Disjunction: _n:t0,1un\u02c6t0,1un\u00d1t0,1unsendspx, yq\u00de\u00d1p maxtai, biuqn\ni\u201c1,\n3.Negation:\u2423m:t0,1um\u00d1t0,1umsends x\u00de\u00d1p1\u00b4ximodp2qqn\ni\u201c1\n4.Ordering:\u010fn:t0,1un\u02c6t0,1un\u00d1t0,1unsendspx, yq\u00de\u00d1p Ixi\u010fyiqn\ni\u201c1,\n5.Ordering (v2): \u011bn:t0,1un\u02c6t0,1un\u00d1t0,1unsendspx, yq\u00de\u00d1p Ixi\u011byiqn\ni\u201c1,\n6.Equality\u201dn:t0,1un\u02c6t0,1un\u00d1t0,1unsendspx, yq\u00de\u00d1p Ixi\u201cyiqn\ni\u201c1.\nTropical Operations Tropical gates are typical in several dynamic programming implementa-\ntions; see e.g. Jukna (2023). We list some key tropical gates which are often considered; and are\nconsidered herein.\n1.Minimum: min :Rn\u00d1Rsend x\u00de\u00d1min 1\u010fj\u010fnxj,\n2.Maximum: max : Rn\u00d1Rsend x\u00de\u00d1max 1\u010fj\u010fnxj,\n3.Median: median : Rn\u00d1Rsends any xPRnto\nmedianpx1, x2,\u00a8\u00a8\u00a8, xnqdef.\u201c#\nxpn`1q{2 ifnis odd\npxpn{2q`xpm{2`1qq{2 if nis even(12)\nwheretxpiqun\ni\u201c1\u201ctxiun\ni\u201c1andxp1q\u010f\u00a8\u00a8\u00a8\u010f xpnq.\n4.Majority Vote: Majn:t0,1un\u00d1t0,1umaps each Boolean xPt0,1udto\nMajnpxqdef.\u201c#\n1 if\u0159n\ni\u201c1xi\u011bn\n2\n0 else .\n20\n\n--- Page 21 ---\nSystematic Neural Network Representations of Algorithms\nHigher-Order Gates Note that several other logical gates, such as strict inequalities, can be\nexpressed in terms of our elementary logical operations. Several of these will be presented as\nexamples and applications of our results below.\nA.2 Logical Gates\nA.2.1 Propositional (Zeroth-Order) Logic\nWe begin by showing that a simple ReLU MLP can implement the vectorized equality verification\ngate; i.e. the maps any pairs a, bPt0,1uBto\nEQUALpa, bqdef.\u201c`\nIai\u201cbi\u02d8B\ni\u201c1.\nThis will allow us to test truth values of statements.\nLemma 2 (ReLU MLP Representation of EQUAL )For any BPN, we have\nEQUALpa, bq\u201c\u03d5EQUALpa, bq\u201cIBpReLUp1B\u00b4ReLUpReLUpA\u00b4\n2,Bpa, bqJq`ReLUpA\u00b4\n2,B\u03a02,Bpa, bqJqqqq\nThus, there exists a bitwise ReLU neural network \u03c8:t0,1uB\u02c6t0,1uB\u00d1t0,1uBof depth 3, width\n2B, and 9B non-zero parameters, such that \u03c8pa, bq\u201cEQUALpa, bq.\nProof Fora, bPt0,1uB, we have that a\u201cbif and only if both a\u010fbandb\u010fa. Equivalently, if\nReLUpa\u00b4bq`ReLUpb\u00b4aq\u201c0. Define\n\u03a6\u201cpa, bq\u201cIBpReLUp1B\u00b4ReLUpReLUpa\u00b4bq`ReLUpb\u00b4aqqqq\n\u201cIBpReLUp1B\u00b4ReLUpReLUpA\u00b4\n2,Bpa, bqJq`ReLUpA\u00b4\n2,B\u03a02,Bpa, bqJqqqq\nThus, a\u201cbiff \u03a6\u201cpa, bq\u201c1B. For each a, bPt0,1uB, we have that a\u201cbif and only if both: a\u010fb\nandb\u010fa; or equivalently, min tReLUpa\u00b4bq,ReLUpb\u00b4aqu\u201c 0. Thus,\nmintReLUpa\u00b4bq,ReLUpb\u00b4aqu\u201c mintReLUpA2,Bpb, aqJq,ReLUppA\u00b4\n2,Bpa, bqJqu (13)\nAs shown, for example in (Petersen and Zech, 2024, Lemma 5.11), the minimum function min t\u00a8,\u00a8u\ncan be realized as the following depth 3 MLP of width Bover all x, yPRB\nmintx, yu\u201cReLUpyq\u00b4ReLUp\u00b4yq\u00b4ReLUpy\u00b4xq. (14)\nCombining (13) with (14) shows that the following equals to the zero vector\n1B\u00b4\u00b4\nReLUpReLUpA2,Bpa, bqJqq\u00b4ReLUp\u00b4ReLUpA2,Bpa, bqJqq\n\u00b4ReLUpReLUpA2,Bpa, bqJq\u00b4ReLUppA\u00b4\n2,B\u03a02,Bpa, bqJqq\u00af (15)\nif and only if a\u201cb. Now, using Lemma 4 we know that the NOTgate can be represented by\nNOTpaq\u201cReLUp1B\u00b4aq. Post-composing the ReLU MLP in (15) by our NOTgate yields the con-\nclusion.\nThe NAND operation is functionally complete, meaning that every possible truth table can be\nexpressed entirely in terms of NANDs. We therefore, represent NAND using a small ReLU MLP.\n21\n\n--- Page 22 ---\nKratsios, Zvigelsky, and Hart\nLemma 3 (ReLU MLP Representation of NAND)For any BPN, we have\nNANDpa, bq\u201c\u03d5NANDpa, bqdef.\u201cIBReLU`\n1B\u00b4IBReLUpA2,Bpa, bqJ\u00b41Bq\u02d8\nThus, there exists a bitwise ReLU neural network \u03c8:t0,1uB\u02c6t0,1uB\u00d1t0,1uBof depth 2, width\nB, and 6Bnon-zero parameters, such that \u03c8pa, bq\u201cNANDpa, bq.\nProof For any a, bPt0,1uBby Lemma 4, NOTpaq\u201cReLUp1\u00b4aqand by Lemma 5 ANDpa, bq\u201c\nReLUpa`b\u00b41q. Together, we have that\nNANDpa, bq\u201cReLU`\n1B\u00b4ReLUpa`b\u00b41Bq\u02d8\n\u201cReLU`\n1B\u00b4ReLUpA2,Bpa, bqJ\u00b41Bq\u02d8\n.\nExamples of Other Propositional Logic Gates Implementable by ReLU MLPs Since\nNAND is functionally complete, every propositional logical gate can be canonically represented in\nterms of NAND gates. Nevertheless, we provide examples of how many of the basic logical gates can\nbe represented (possibly more efficiently) directly without relying on the NAND gate construction\nabove; note that, using these will no longer yield a canonical construction but will yield smaller\nReLU MLP encodings of algorithms.\nLemma 4 (Neural network representation of NOT)For any BPN,aPt0,1uB, we have\nNOTpaq\u201cIBReLUp1B\u00b4IBaq.\nThus, there exists a bitwise ReLU neural network \u03c8:t0,1uB\u00d1t0,1uBof depth 1, width width B,\nand3Bnon-zero parameters, such that \u03c8paq\u201cNOTpaq.\nLemma 5 (Neural network representation of AND)For any BPN,a, bPt0,1uB, we have\nANDpa, bq\u201cIBReLUpA2,Bpa, bq\u00b41Bq.\nThus, there exists a bitwise ReLU neural network \u03c8:t0,1uB\u02c6t0,1uB\u00d1t0,1uBof depth 1, width\nB, and 4Bnon-zero parameters, such that \u03c8pa, bq\u201cANDpa, bq.\nLemma 6 (Neural network representation of OR)For any BPN,pa, bqPt0,1u2B, we have\nORpa, bq\u201cIB`\nReLUp1B\u00b4ReLUp\u00b4A2,Bpa, bqJ`1Bqq\u02d8\n.\nThus, there exists a bitwise ReLU neural network \u03c8:t0,1uB\u02c6t0,1uB\u00d1t0,1uBof depth 2, width\nB, and 6Bnon-zero parameters, such that \u03c8pa, bq\u201cORpa, bq.\nProof For any a, bPt0,1uB,ORpa, bq\u201c1\u00b4ReLUp1\u00b4a\u00b4bq\u201c1B\u00b4ReLU`\n\u00b4A2,Bpa, bqJ`1B\u02d8\n.\nLemma 7 (Neural network representation of XOR)For any BPN,pa, bqPt0,1u2B, we have\nXORpa, bq\u201cIB`\nReLUpReLUpA2,Bpa, bqJq\u00b42ReLUpA2,Bpa, bqJ\u00b41Bqq\u02d8\n.\nThus, there exists a bitwise ReLU neural network \u03c8:t0,1uB\u02c6t0,1uB\u00d1t0,1uBof depth 2, width\n2B, and with 8Bnon-zero parameters such that \u03c8pa, bq\u201cXORpa, bq.\n22\n\n--- Page 23 ---\nSystematic Neural Network Representations of Algorithms\nProof Note that, for each a, bPt0,1uBwe have\nXORpa, bq\n\u201cReLUpReLUpa`bq\u00b42ReLUpa`b\u00b41qq\n\u201cReLUpReLUpA2,Bpa, bqJq\u00b42ReLUpA2,Bpa, bqJ\u00b41Bqq.\nLemma 8 (Neural network representation of IMPLY )For any BPN,pa, bqPt0,1u2B\nIMPLYpa, bq\u201cIB`\nReLUp1B\u00b4A2,Bpa, bqq\u02d8\n.\nThus, there exists a bitwise ReLU neural network \u03c8:t0,1uB\u02c6t0,1uB\u00d1t0,1uBof depth 1, width\nB, and with 4Bnon-zero parameters such that \u03c8pa, bq\u201cIMPLYpa, bq.\nProof The fact that IMPLY\u201cNOT\u02ddORand Lemma 4 and Lemma 6 imply that\nIMPLYpa, bq\u201cNOT\u02ddORpa, bq\n\u201cIBReLUp1B\u00b4A2,Bpa, bqq,\nsince ReLU\u02ddReLU\u201cReLU.\nA.3 Predicate (First-Order) Logic\nSince we know that our MLPs can implement the NOTgate, then the identity \u2423p@\u2423 Pq\u201cD Pfor\nany given proposition Pimplies that we can obtain all our first-order logical operations simply by\nimplementing @quantifiers applied to computable propositions P. By computable, we simply mean\nthat the proposition has itself already been represented by a ReLU MLP. The next proposition\nshows that this is indeed possible; however, as one may expect, the resulting ReLU MLP unit is\nrather large.\nLemma 9 (Sentence Verification) LetB1, B2PN`and\u03d5:RB1`B2\u00d1Ra ReLU MLP of depth\nD, Width W, and with Snon-zero weights. Then there exists a ReLU MLP \u03a6@\u201cp\u00a8|\u03d5q:RB2\u00d1R\nsatisfying: for all yPt0,1uB2\n`\n@xPt0,1uB1. \u03d5px, yq\u201c1\u02d8\n\u00f4\u03a6@\u201cpy|\u03d5q\u201c1.\nMoreover, \u03a6@\u201cp\u00a8|\u03d5qhas depth B1`D`1, width at-most 2 maxt2B1W,2B13u, and with at-most\n2p2B1S`2B1`3\u00b49qnon-zero weights.\nProof LetxPt0,1uB1andyPt0,1uB2. We have that\n@xPt0,1uB1. \u03d5px, yq\u201c1\u00f4min\n2B1\u00e0\nxPt0,1uB1ReLUp\u03d5px, yqq\u201c1 (16)\n23\n\n--- Page 24 ---\nKratsios, Zvigelsky, and Hart\nNote that the cardinality of t0,1uB1is 2B1. Applying (Petersen and Zech, 2024, Lemma 5.11),\nwe know that there is a ReLU MLP \u03a6min\n2B1:R2B1\u00d1Rwith\nsizep\u03a6min\n2B1q\u010f16 2B1,widthp\u03a6min\n2B1q\u010f2B13,and depthp\u03a6min\n2B1q\u010f rlog2p2B1qs\u201cB1\nsatisfying: for each x1, . . . , x2B1PRwe have\n\u03a6min\n2B1px1, . . . , x2B1q\u201c min\n1\u010fj\u010f2B1xj. (17)\nCombining (17) with (16) we see that the network: \u03a6 : RB2\u00d1Rgiven by\n\u03a6pyqdef.\u201c\u03a6min\n2B1\u02dd\u03a6id\n1\u02dd\u00a8\n\u02dd\u00e0\nxPt0,1uB1ReLUp\u03d5px, yqq\u02db\n\u201a,\nhas a depth of B1`D`1, width at-most 2 max t2B1W,2B13u, and with at-most 2 p2B1S`2B1`3\u00b49q\nnon-zero weights. Further, for yPt0,1uB2\n\u03a6pyq\u201c min\nxPt0,1uB1ReLUp\u03d5px, yq\u00b41q.\nSince, for any given yPt0,1uB2, \u03a6py|\u03d5q\u201c1 if and only if \u03a6 pyq\u201c1 then we are done.\nWe remark that higher order logic, where, for instance, statements can quantify over subsets of\nt0,1uB1, in much the same way as Lemma 9. All of this, of course, is contingent on the finiteness\nof the structures we are considering.\nA.4 Analytic Gates\nLemma 10 (Constant Functions) LetdPN`andcPRd\nq. Define the ReLU MLP \u03a6c:Rd\u00d1tcu\nfor each xPRdby\n\u03a6cpxqdef.\u201c p\u00b4 In, InqReLUp02n\u02c6nx`p\u00b4c, cqJq,\nwhere \u03a6cpxq \u201c cfor all xPRd. Furthermore, \u03a6cpxqhas depth 1, width 2d, and 4dnon-zero\nparameters.\nProof By construction.\nLemma 11 (ReLU MLP Implementation of Indicator of ra,8q)FixqPN`,aPRq, and\ndefine the ReLU MLP \u03a6ra,8q:q:R\u00d1r0,1sfor each xPRby\n\u03a6ra,8q:qpxq\u201cReLU`\n1\u00b4ReLU`\n\u00b42q`1px\u00b4aq\u02d8\u02d8\nof depth 2, width 1, with 5non-zero parameters. Then \u03a6r0,8q:q\u201eqIra,8q.\nProof [Proof of Lemma 11] If xPra,8qthen \u03a6r0,8q:qpxq\u201c1 and if xPp\u00b48 , a\u00b42\u00b4pq`1qsthen,\n\u03a6r0,8q:qpxq\u201c0.\nBy a similar token, we may construct the indicator function of the open complementary interval.\n24\n\n--- Page 25 ---\nSystematic Neural Network Representations of Algorithms\nLemma 12 (ReLU MLP Implementation of Indicator of p\u00b48, aq)FixqPN`,aPRq, and\ndefine the ReLU MLP \u03a6p\u00b48,aq:q:R\u00d1r0,1sfor each xPRby\n\u03a6p\u00b48,aq:qpxq\u201cReLU`\n1\u00b4ReLU`\n2q`1px\u00b4aq`1\u02d8\u02d8\nof depth 2, width 1, with 5non-zero parameters. Then \u03a6r0,8q:q\u201eqIp\u00b48,aq.\nProof [Proof of Lemma 12] If xPp\u00b48 , a\u00b42\u00b4pq`1qsthen \u03a6p\u00b48,aq:qpxq\u201c1 and if xPra,8qthen\n\u03a6p\u00b48,aq:qpxq\u201c0.\nLemma 13 (ReLU MLP Implementation of Indicator Functions of ra, bs)Leta, bPRqwith\na\u010fb. For every qPN`consider the ReLU MLP \u03a6ra,bs:q:R\u00d1Rwith depth 2, width 2, and 9\nnon-zero parameters defined for each xPRby\n\u03a6ra,bs:qpxqdef.\u201c p1,\u00b41qReLU`\n\u00b4ReLU`\np\u00b42q`1,\u00b42q`1qJx`p2q`1a,2q`1b`2\u00b4q\u00b41qJ\u02d8\n`p1,1qJ\u02d8\n.\nThen, \u03a6ra,bs:qpxq\u201eqIra,bs.\nProof [Proof of Lemma 13] By construction, we have \u03a6 ra,bs:qpxq\u201cReLU`\n\u03a6ra,8q:qpxq\u00b4\u03a6rb`2\u00b4pq`1q,8q:qpxq\u02d8\n.\nNow, Lemmas 11 and 12 imply that Ira,8q\u00b4Irb`2\u00b4pq`1q,8q\u201eq\u03a6ra,8q:q\u00b4\u03a6rb`2\u00b4pq`1q,8q:q. However,\nIrb`2\u00b4pq`1q,8q\u201eqIpb,8q, and so,\nIra,bs\u201eqIra,8q\u00b4Ipb,8q\u201eqIra,8q\u00b4Irb`2\u00b4pq`1q,8q\u201c\u03a6ra,bs:qpxq\nconcluding our proof.\nA direct consequence of Lemma 13 and the \u201eqrelation is the following indicator function.\nCorollary 6 (Indicator of Half-Open Intervals) Leta, bPRqwith a\u010fb. For every qPN`\nconsider the ReLU MLP \u03a6ra,bq:q:R\u00d1Rwith depth 2, width 2, and 9non-zero parameters defined\nfor each xPRby\n\u03a6ra,bq:qpxqdef.\u201c p1,\u00b41qReLU`\n\u00b4ReLU`\np\u00b42q`1,\u00b42q`1qJx`p2q`1a,2q`1pb\u00b42\u00b4pq`1qq`2\u00b4pq`1qqJ\u02d8\n`p1,1qJ\u02d8\n.\nThen, \u03a6ra,bq:q\u201eqIra,bq.\nProof [Proof of Corollary 6] Since Ira,bq\u201eqIra,b\u00b42\u00b4pq`1qsthen the result follows from Lemma 13\nupon setting \u03a6 ra,bq:qdef.\u201c\u03a6ra,b`1\n2q`1s:q.\nWe also obtain the following useful construction directly.\nCorollary 7 (Indicator of Complements to Half-Open Intervals) Leta, bPRqwith a\u010fb.\nFor every qPN`consider the ReLU MLP \u03a6ra,bqc:q:R\u00d1Rwith depth 2, width 2, and 10non-zero\nparameters defined for each xPRby\n1\u00b4p1,\u00b41qReLU`\n\u00b4ReLU`\np\u00b42q`1,\u00b42q`1qJx`p2q`1a,2q`1pb\u00b42\u00b4pq`1qq`2\u00b4pq`1qqJ\u02d8\n`p1,1qJ\u02d8\nThen, \u03a6ra,bqc:q\u201eqIRzra,bq.\n25\n\n--- Page 26 ---\nKratsios, Zvigelsky, and Hart\nProof [Proof of Corollary 7] By construction \u03a6 ra,bqc:q\u201c1\u00b4\u03a6ra,bq:q; and so the result follows from\nCorollary 6.\nLemma 14 (Indicator of a Single Point in Rd)For each d, qPN`, there is a ReLU MLP\n\u03a6a,q:Rd\u00d1Rdwith width at-most 4d, depth 4, and 18d`5non-zero parameters satisfying\n\u03a6a,qpxq\u201cIx\u201ca\nfor each xPRd\nq.\nProof For each a\u201cpa1,...,adq\n2qPRd\nq, and each i\u201c1, . . . , d , let Consider the real-valued ReLU MLP,\nmapping any xPRto\n\u03a6ai,qpxqdef.\u201c1 ReLU\u00b4\np\u00b42q`1,\u00b42q`1qJReLU`\np1,\u00b41qx``\n\u00b42qa,2qa\u02d8\u02d8\n`1\u00af\n.\nThen, for each xPRq, we have\n\u03a6ai,qpxq\u201c#\n1 if x\u201ca\n2q\n0 else .\nFurthermore, \u03a6 ai,qhas depth 2, width 2, and 8 non-zero parameters. Now, applying the paralleliza-\ntion Lemma in (Petersen and Zech, 2024, Lemma 5.3), there is a ReLU MLP \u02dc\u03a6a,q:Rd\nq\u00d1Rdwith\nwidth at-most 4 d, depth 2, and 16 d`2d\u201c18dnon-zero parameters satisfying\n\u02dc\u03a6a,qpxq\u201c1J\nd`\n\u03a6a1,qpeJ\n1xq, . . . , \u03a6ad,qpeJ\ndxq\u02d8\n\u201cd\u00ff\ni\u201c1\u03a6ad,qpxiq\nfor all xPRd.\nThough one can proceed from this point by using the multiplication lemma, namely Lemma 31,\nthe following construction is significantly more efficient: Note that, for each xPRd\nq, \u03a6a,qPr0, ds\nwith value dbeing achieved if and only if x1\u201ca1\n2q, . . . , x d\u201cad\n2q. Next, observe that the \u201c d-threshold\u201d\nReLU MLP: \u03a6 Th:d,q:R\u00d1Rgiven for each xPRby\n\u03a6Th:d,qpxqdef.\u201c1 ReLU`\n\u00b42q`1ReLUp\u00b4x`dq`1\u02d8\nsatisfies the following for each xPRq\n\u03a6Th:d,qpxq\u201c#\n1 if x\u011bd\n0 else .\nMoreover, \u03a6 Th:d,qhas depth 2, width 1, and 5 non-zero parameters. Thus, the desired map\n\u03a6a,q:Rd\u00d1Ris given by \u03a6 a,qdef.\u201c\u03a6Th:d,q\u02dd\u02dc\u03a6a,q.\n26\n\n--- Page 27 ---\nSystematic Neural Network Representations of Algorithms\nBit Extractor\nWe now show how a ReLU MLP can extract a binary representation of any number in Rd\nq.\nLemma 15 (ReLU MLP Implementation of the Integer Floor Representation) Let t\u00a8u:\nR\u00d1Zdenote the integer ceiling function. For any MPN`there exists a ReLU MLP \u03a6t\u00a8u:M:\nR\u00d1ZXr\u00b4M, Mswith depth at-most rlog2p2M`1qs`3, width 16M`8, and 108M`54non-zero\nparameters. such that\n\u03a6t\u00a8u:Mdef.\u201c\u03a6min\n2M`1\u02dd\u03a6\u2039\nq\u201eqt\u00a8u\nwhere \u03a6min\n2M`1\u201cmini\u201c\u00b4M,...,M xiis constructed in (Petersen and Zech, 2024, Lemma 5.11) and\n\u03a6\u2039\nqpxqdef.\u201cM\u00e0\nn\u201c\u00b4M`\nM`1\n2q`1\u02d8\n\u03a6r0,1qc:qpn\u00b4xq`n\u03a6r0,1q:qpn\u00b4xq,\nand\u03a6r0,1s:qand\u03a6r0,1sc:qare constructed in Corollaries 6 and 7, respectively.\nProof [Proof of Lemma 15] First, observe that: for each xPr\u00b4M, Mswe have\ntxu\u201cmin\u2423\nn In\u00b4xPr0,1q`pM`2\u00b4pq`1qqIn\u00b4xRr0,1q:nPr\u00b4M, MsXZ(\n. (18)\nBy Corollary 6, there is a ReLU MLP \u03a6 r0,1q:q:R\u00d1Rsatisfying \u03a6 r0,1q:q\u201eqIr0,1qwith depth 2, width\n2, and 9 non-zero parameters. Similarly, by Corollary 7 there is a ReLU MLP \u03a6 r0,1qc:q:R\u00d1R\ngiven by \u03a6 r0,1qc:qdef.\u201c1\u00b4\u03a6r0,1q:qsatisfying \u03a6 r0,1qc:q\u201eqIRzr0,1qwhich has with depth 2, width 2, and\n10 non-zero parameters. Stacking those networks and rescaling, we find that the function\npM`1\n2q`1qIpn\u00b4xqPRzr0,1q`nIpn\u00b4xqPr0,1q\u201eqpM`1\n2q`1q\u03a6r0,1qc:qpn\u00b4xq`n\u03a6r0,1q:qpn\u00b4xq\u201c\u03a6M,n:q(19)\nConsequently, we may parallelize the above construction to obtain the 2 layer ReLU MLP \u03a6\u2039\nq:R\u00d1\nRsending any xPRto\n\u03a6\u2039\nqpxqdef.\u201cM\u00e0\nn\u201c\u00b4M\u03a6M,n:qpxq.\nMoreover, \u03a6\u2039\nqhas depth 3, width 16 M`8, and 76 M`38 non-zero parameters. Appealing to (Pe-\ntersen and Zech, 2024, Lemma 5.11), there exists a ReLU MLP \u03a6min\n2M`1:R2M`1\u00d1Rimplementing\n\u03a6min\n2M`1px1, . . . , x 2M`1q\u201c min\nm\u201c1,...,2M`1xm (20)\nfor all xPR2M`1where \u03a6min\n2M`1has depth at-most rlog2p2M`1qs, width at-most 6 M`3, and\nat-most 32 M`16 non-zero parameters. Define\n\u03a6t\u00a8u:Mdef.\u201c\u03a6min\n2M`1\u02dd\u03a6\u2039\nq.\nConsequentially, (19) and (20) imply that (18) can be represented as\n\u03a6t\u00a8u:M\u201et\u00a8u.\nBy construction \u03a6 t\u00a8u:Mwhich has depth at-most rlog2p2M`1qs`3, width 16 M`8, and 108 M`54\nnon-zero parameters.\n27\n\n--- Page 28 ---\nKratsios, Zvigelsky, and Hart\nA.5 Bit Encoder and Decoder Networks\nIn this section, we explicitly construct a mapping between our dictionary of elementary operations\nG0to a fixed set of elementary MLPs. It will often be convenient to move to, and from, binary\nrepresentations of numbers in Rqof the form\nx\u201cx0loomoon\nsignx1. . . x Blooomooon\nsignificant digitsxB`1. . . x B`e looooooomooooooon\nexponent. (21)\nFor example, for double precision (64bits), one has 53 bits for significand, 11 bits for exponent, and\n1 sign bit. We now formalize a ReLU MLP encoder sending any xPRqto the binary floating-point\ntype representation on the right-hand side of (21) and a ReLU MLP decoder which reverses this\nprocedure. The binary representation in (21) implies two maps. The first encodes every real\nnumber in Rqinto its binary expansion; that is, given any q, MPN`, the bit encoder map\nBinM:q:Rq\u00d1t0,1u2q`2\nBinM:qpxq\u00de\u00d1 p \u03b2iqq\ni\u201c0loomoon\nInteger Part\u2018 p\u03b2iq\u00b41\ni\u201c\u00b4qlooomooon\nFractional Part\u2018Ix\u011b0loomoon\nSign(22)\nwhere x\u201c\u0159q\ni\u201c\u00b4q\u03b2i2iis the unique bit representation, and sign, of any xPRq. Next, the simpler\nbit decoder inverts the bit encoder Bin M:qby sending any such binary representation t0,1u2q`2back\nto the real number in Rqit encodes. Given any M, qPN`, we define the bit decoder map\nBitM:q:t0,1u2q`2\u00d1Rq\nBitM:qpp\u03b2iq2q`2\ni\u201c0q\u00de\u00d1p\u00b4 1qq2`2\u02c6q\u00ff\ni\u201c0\u03b2i2i`\u00b41\u00ff\ni\u201c\u00b4q\u03b2q`1`i2\u00b4i\u02d9\n.(23)\nWe now construct networks emulating both of these maps.\nA.5.1 Bit Decoder\nProposition 1 FixM, qPN`. Then, there is a ReLU MLP \u03a6Bit,M :q:R2q`2\u00d1Rof depth 1,\nwidth 2q`2, and with at-most 2q`10non-zero parameters such that: for every \u03b2Pt0,1u2q`2we\nhave BitM,qp\u03b2q\u201c\u03a6Bit,M :qp\u03b2q.\nProof [Proof of Proposition 1] Consider the linear map gpuqdef.\u201c \u00b42u`1 onR; note that gp0q\u201c1\nandgp1q\u201c\u00b4 1. Now, consider the 2 \u02c6p2q`2qmatrix\nD1def.\u201c\u00a8\n\u02ddp2iqq\ni\u201c0\u2018p2\u00b4iq\u00b4q\ni\u201c\u00b41\u201801\n02q`1\u2018p\u00b4 2q\u02db\n\u201a\nand the bias vector d 1def.\u201c02q`1\u2018p1q; the total number of non-zero entries in D 1and d 1is 2q`2.\nNow, for any \u03b2def.\u201c p\u03b2iq2q`2\ni\u201c1with xdef.\u201cBitM:qp\u03b2qwe have\nD1\u03b2`d1\u201cpy, zqJdef.\u201c`\n|x|,sgnpxq\u02d8J(24)\nwhere, we recall that sgn pxq\u201c1 ifx\u01050, sgnpxq\u201c\u00b4 1 ifx\u01030, and sgnp0q\u201c0.\n28\n\n--- Page 29 ---\nSystematic Neural Network Representations of Algorithms\nNow consider the 2 \u02c62 matrix D 2and the vector d 2PR2given by\nD2def.\u201c\u00a8\n\u02dd1 M`1\n2q`1\n\u00b41\u00b4M\u00b41\n2q`1\u02db\n\u201aand d 2def.\u201c`\n\u00b4M`1\n2q`1, M`1\n2q`1\u02d8J.\nWe readily verify that the ReLU MLP \u03a6 Bit,M :q:R2q`2\u00d1Rdefined for any xPR2q`2by\n\u03a6Bit,M :qp\u03b2qp1,1qReLU\u00b4`\nD2D1x`d1\u02d8\n`d2\u00af\n(25)\nis such that: if \u03b2Pt0,1u2q`2andx\u201cBitM,qp\u03b2qthen\n\u03a6Bit,M :qp\u03b2q\u201cp1,1qReLU\u02c6\nD2`\n|x|,sgnpxq\u02d8J`d2\u02d9\n\u201c$\n\u2019\u2019&\n\u2019\u2019%\u02c6\u0159q\ni\u201c0\u03b2i`q`12i`\u0159\u00b41\ni\u201c\u00b4q\u03b2i`q`12\u00b4i\u02d9\n: ifx\u011b0\n\u02c6\u0159q\ni\u201c0\u03b2i`q`12i`\u0159\u00b41\ni\u201c\u00b4q\u03b2i`q`12\u00b4i\u02d9\n: ifx\u01030\n\u201cBitM,qp\u03b2q\u201cx.\nMoreover, \u03a6 Bit,M :qhas depth 1, width 2 q`2, and at-most 2 q`10 non-zero parameters.\nA.5.2 Bit Encoder\nWe begin by emulating the bit encoder map in (22), by a feedforward ReLU neural network, the\nconstruction of which is undertaken via the next few lemmata. We note that more complicated\nbit-extraction networks have previously been constructed in the learning theory literature (see, e.g.,\nBartlett et al. (1998, 2019)). Here, we prefer a simple and direct construction that could, in principle,\nbe easily implemented, and we extract the sign information as well. We first construct a ReLU\nMLP, which (uniquely) decomposes any number in Rqinto: its positive integer part, its remainder\ninp\u00b41,1q, and identifies its sign. Once we have computed these quantities, it only remains to apply\na long division algorithm on the remainder and integer parts to obtain the binary representation of\nany such number.\nLemma 16 (ReLU MLP Implementation: Sign-Integer-Remainder Decomposition) Fix\nqPN`. There exists a ReLU MLP \u03a6Bit:M:`:R\u00d1R3of depth at-most rlog2p2M`1qs`4, width\nat-most 32M`22, and at-most 216M`134non-zero parameters such that: for each xPRq\n\u03a6Bit:M:`pxq\u201c\u00b4\nt|x|uloomoon\nn:integer part,|x|\u00b4 t|x|uloooomoooon\nr:remainder, Ix\u011b0loomoon\ns:sign\u00afJ\nwhere x\u201cspn`rq.\nProof [Proof of Lemma 16] The absolute value function RQx\u00de\u00d1|x|Pr0,8qcan be implemented\nby the following ReLU MLP \u03a6 |\u00a8|:R\u00d1 r0,8qof depth 1, width 2, with 4 non-zero parameters;\ndefined for each xPRby\n\u03a6|\u00a8|pxq\u201cReLUpxq`ReLUp\u00b4xq\u201cp1,1qReLU\u00b4\np1,\u00b41qJx\u00af\n.\n29\n\n--- Page 30 ---\nKratsios, Zvigelsky, and Hart\nNow, suppose that xP p0,8q. First note that xcan be uniquely written as x\u201cn`rwith\nnPN0andrP r0,1qa remainder. Moreover, n\u201ctxuandr\u201cx\u00b4txu. Let \u03a6 t\u00a8u,Mdenote the\nReLU MLP of Lemma 15 and let \u03a6 r0,8q:qdenote the ReLU MLP of Lemma 11. Consider the map\n\u03a6Bit:M:`:R\u00d1R2given for each xPRqwith x\u201cn`r,nPN0, and rPr0,1q, by\n\u03a6Bit:M:`pxqdef.\u201c\u00a8\n\u02da\u02da\u02dd0 1 0\n1\u00b41 0\n0 0 1\u02db\n\u2039\u2039\u201a\u00a8\n\u02da\u02da\u02dd\u03a6|\u00a8|\n\u03a6t\u00a8u:M\u02dd\u03a6|\u00a8|pxq\n\u03a6r0,8q:qpxq\u02db\n\u2039\u2039\u201a\u201c\u00a8\n\u02da\u02da\u02ddt|x|u\n|x|\u00b4 t|x|u\nIx\u0103\u00b42\u00b4q\u02db\n\u2039\u2039\u201a\u201c\u00a8\n\u02da\u02da\u02ddn\nr\nIxPr\u00b41{2q,8q\u02db\n\u2039\u2039\u201a.\nThe parallelization lemma in (Petersen and Zech, 2024, Lemma 5.3) implies that \u03a6 Bit:M:`can be\nrepresented as a ReLU MLP of depth at-most rlog2p2M`1qs`4 width at-most 32 M`22 and\nat-most 216 M`134 non-zero parameters.\nUsing Lemma 16, we only need to express two bit-extraction subroutines: 1) which obtains the\nbinary representation of any natural number and 2) which obtains the binary representation of any\n\u201cremainder\u201d in r0,1q.\nLemma 17 (Bit Encoder - Remainder Only) FixqPN`. Consider ReLU MLP \u03a6Rem: q:R\u00d1\nRqof depth of rlog2p2M`1qs`3, a width at-most 2qp16M`8q, and no more than 216Mq`111q`1\nand defined by\n\u03a6Rem: qpxqdef.\u201c pIq\u00b42Lq\u03a6}q\nt\u00a8u:M`\ndiagpp2iqq\ni\u201c1qx\u02d8\nwhere \u03a6}q\nr\u00a8s:Mis the q-fold parallelization of the ceiling network \u03a6t\u00a8u:Min Lemma 15. Then, \u03a6Rem: q\nis such that: for each rPp0,1qXRqwe have\np\u03b21, . . . , \u03b2 qqdef.\u201c\u03a6Rem: qprqandr\u201cq\u00ff\nm\u201c1\u03b2m2\u00b4q.\nProof LetrPr0,1q; then, r\u201c\u01598\ni\u201c1\u03b2i2\u00b4ifor somep\u03b2iq8\ni\u201c1inr0,8q. Set \u03b30def.\u201c0 and for each\niPN`define \u03b3idef.\u201ct2iru. Then, for each iPN`, we may express each \u03b2iby\n\u03b2i\u201c\u03b3i\u00b42\u03b3i\u00b41 (26)\nMoreover, \u03b2iPt0,1ufor each iPN`.\nLetAdef.\u201c p2i\u03b4i,jqq\ni,j\u201c1def.\u201cdiagpp2iqq\ni\u201c1qwhere \u03b4i,jis the Kronecker delta and let 1qPRqdenote\nthe vector with all entries equal to 1. Consider the map \u03a6 rem:q:R\u00d1Rqgiven for each xPRby\n\u03a6rem:qpxqdef.\u201c pIq\u00b42LqtAxu (27)\nwhere t\u00a8uis applied componentwise and where Lis the q\u02c6qleft-shift matrix defined in (30). Note\nthat the matrix pIq\u00b42Lqhas exactly 2 q\u00b41 non-zero parameters. Then, for each rPRqXr0,1q,\nwe have that\n\u03a6rem:qprq\u201c`\n\u03b2m\u02d8q\nm\u201c1(28)\nwhere \u03b21, . . . , \u03b2 qPt0,1uare given by (26).\nNow, by Lemma 15 the network \u03a6 t\u00a8u:M\u201eqt\u00a8uhas depth at-most rlog2p2M`1qs`3, width 16 M`8,\n30\n\n--- Page 31 ---\nSystematic Neural Network Representations of Algorithms\nand 108 M`54 non-zero parameters. By the parallelization Lemma in (Petersen and Zech, 2024,\nLemma 5.3), there is a ReLU MLP \u03a6\u2225q\nt\u00a8u:M:R\u00d1Rqof depth at-most rlog2p2M`1qs`3, width\nat-most 2 qp16M`8q, and with at-most 2 qp108M`54qnon-zero parameters, satisfying: for each\nxPRq, \u03a6\u2225q\nt\u00a8u:Mpxq\u201c\u00c0q\ni\u201c1\u03a6t\u00a8u:Mpxq. Therefore, (27) can be represented as \u03a6 Rem: q\u201eq\u03a6rem:qwhere,\nfor every xPRwe define\n\u03a6Rem: qpxqdef.\u201c pIq\u00b42Lq\u03a6\u2225q\nt\u00a8u:MpAxq.\nConsequently, \u03a6 Rem: qhas a depth of rlog2p2M`1qs`3, a width at-most 2 qp16M`8q, and no more\nthan 216 Mq`111q`1 non-zero parameters.\nNext, we construct the bit encoder for integers in bounded sub-intervals of the positive real numbers.\nLemma 18 (Bit Encoder Integer Part Only) FixMPN`and consider the map BinInt:q:\nNXr0, Ms \u00de\u00d1 t 0,1u1`qsending any nPNXr0, Msto the coefficients p\u03b2iqq\ni\u201c0P t0,1u1`qof its\nunique binary expansion; i.e. n\u201c\u0159q\ni\u201c0\u03b2i2i. There exists a ReLU MLP \u03a6Int:q:R\u00d1Rqsatisfying\n\u03a6Int:q\u201eqBinInt:q.\nMoreover, \u03a6Int:qhas depth of rlog2p2M`1qs`3, width at-most 2pq`1qp16M`8q, no more than\n216Mq`108q`216M`108non-zero parameters, and is represented for each xPRby\n\u03a6Int:qpxqdef.\u201c pI1`q\u00b42 R1`qq\u03a6}q\nt\u00a8u:M`\ndiagpp2iqq\ni\u201c1qx\u02d8\nwhere R1`qis the square 1`qright-shift matrix and \u03a6}1`q\nt\u00a8u:Mis thep1`qq-fold parallelization of the\nfloor network \u03a6t\u00a8u:Min\nProof [Proof of Lemma 18] The proof is nearly identical to that of Lemma 17. First, notice that\nfor every integer ninr0, Mswe have\n\u03a6Int:qpnq\u201cpI1`q\u00b42 R1`qqtdiagpp2iqq\ni\u201c0xu\u201c\u00a8\n\u02da\u02da\u02da\u02da\u02da\u02da\u02ddYn\n2m]\n\u00b42Yn\n2m`1]\nYn\n2m\u00b41]\n\u00b42Yn\n2m]\n...Yn\n20]\n\u00b42Yn\n21]\u02db\n\u2039\u2039\u2039\u2039\u2039\u2039\u201a\u201cBinInt:qpnq.\nUpon parameter tallying nearly identically to the tally used in \u03a6 Rem: q, with 1`qin place of qand\nwithout the parameters in the extra bias term 1 q, we obtain the conclusion.\nCombining the above into a single ReLU feedforward neural network performing bit extraction on\nRqyields our result.\nProposition 2 (Bit Encoder) LetM, qPN`. There exists a ReLU feedforward neural network\n\u03a6Bin:M,q:R\u00d1R2q`2satisfying \u03a6Bin:M,q\u201eqBinq. Moreover, \u03a6Bin:M,qhas depth 2rlog2p2M`1qs`7\nwidth 128Mq`64M`64q`36at-most 432Mq`221q`432M`254non-zero parameters, and\nrepresentation\n\u03a6Bin:M,qdef.\u201c`\n\u03a6Int:qpP1\u00a8q,\u03a6Rem: qpP2\u00a8q,\u03a6r0,M`1\n2q`1q:qpP3\u00a8q\u02d8\n\u02dd\u03a6Bit:M:`\nwhere \u03a6Bit:M:`is the ReLU MLP defined in Lemma 16, \u03a6Int:qis the ReLU MLP of Lemma 18,\n\u03a6Rem: qis the ReLU MLP of Lemma 17 , \u03a6r0,M`1\n2q`1q:qis the ReLU MLP of Corollary 7, and\nP1pxqdef.\u201c pxiqq\ni\u201c1,P2pxqdef.\u201c pxiq2q`1\ni\u201cq`1, and P3pxq\u201cx2q`2are projection matrices on R2q`2.\n31\n\n--- Page 32 ---\nKratsios, Zvigelsky, and Hart\nProof [Proof of Proposition 2] By construction, i.e. from the quoted lemmata, \u03a6 Bin:M,qsatisfies\n\u03a6Bin:M,q\u201eBinM:q. Tallying the number of non-zero parameters in each network, as well as the\n2q`2 non-zero parameters totalled in the projection matrices, yields our parameter count.\nA.5.3 Additional Operations\nWe now compile a list of networks that we will often use to manipulate the bits of strings that\nour networks are processing. These include networks helping with modular arithmetic base 2 and\nnetworks which shift bits along either left or right.\nTo implement multiplication, we must treat i`jmodp2qfor integers\u00b42\u010fi, j\u010f2. We thus\nimplement an MLP which can perform the pmod 2qoperation on this small set of integers.\nLemma 19 Consider the following ReLU MLP \u03a6mod 2 :R\u00d1 r0,1sof width 6, depth 2and24\nnon-zero parameters, defined for each xPRby\n\u03a6mod 2pxqdef.\u201c p1,1,1qReLU\u00a8\n\u02da\u02da\u02da\u02da\u02da\u02da\u02da\u02da\u02da\u02da\u02da\u02dd\u00a8\n\u02da\u02da\u02dd\u00b41\u00b41 0 0 0 0\n0 0\u00b41\u00b41 0 0\n0 0 0 0 \u00b41\u00b41\u02db\n\u2039\u2039\u201aReLU\u00a8\n\u02da\u02da\u02da\u02da\u02da\u02da\u02da\u02da\u02da\u02da\u02da\u02dd16x`\u00a8\n\u02da\u02da\u02da\u02da\u02da\u02da\u02da\u02da\u02da\u02da\u02da\u02dd1\n\u00b41\n\u00b41\n\u00b43\n3\n1\u02db\n\u2039\u2039\u2039\u2039\u2039\u2039\u2039\u2039\u2039\u2039\u2039\u201a\u02db\n\u2039\u2039\u2039\u2039\u2039\u2039\u2039\u2039\u2039\u2039\u2039\u201a`\u00a8\n\u02da\u02da\u02dd1\n1\n1\u02db\n\u2039\u2039\u201a\u02db\n\u2039\u2039\u2039\u2039\u2039\u2039\u2039\u2039\u2039\u2039\u2039\u201a.\n(29)\nThen, for each xPt\u00b42,1,0,1,2u, we have \u03a6mod 2pxq\u201cxpmod 2q.\nProof [Proof of Lemma 19] Consider the map g:R\u00d1Rdefined for each xPRby\ngpxqdef.\u201cReLUp\u00b4ReLUp\u00b4x`1q`1\u00b4ReLUpx\u00b41qq\nand define the map f:R\u00d1Rfor each xPR\nfpxqdef.\u201cgpxq`gpx\u00b42q`gpx`2q\nand note that fp\u00b42q\u201cfp0q\u201cfp2q\u201c0 and fp\u00b41q\u201cfp1q\u201c1. Thus, for each xPt\u00b42,1,0,1,2u,\nwe deduce that fpxq\u201cxpmod 2q.\nTo realize fas a ReLU MLP \u03a6 mod 2 :R\u00d1R. For this note that for each xPRwe see that \u03a6 mod 2,\nas defined in (29), implements mod 2 on{-2,-1,0,1,2 }. This completes our proof.\nFinally, we will rely on the following set of MLPs which implement left and right shifts of the\nbinary representations of integers.\n32\n\n--- Page 33 ---\nSystematic Neural Network Representations of Algorithms\nNeural network representation of LSHIFT A left-shift on t0,1uBis implemented by multipli-\ncation by the following matrix:\nLdef.\u201c\u00bb\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u20130 1 0 0\u00a8\u00a8\u00a80\n0 0 1 0\u00a8\u00a8\u00a80\n0 0 0 1\u00a8\u00a8\u00a80\n0 0 0 0......\n...............1\n0 0 0 0\u00a8\u00a8\u00a80fi\nffiffiffiffiffiffiffiffiffiffiffifl. (30)\nLemma 20 (Left Shift) For any BPN,aPt0,1uBwe have IBReLUpLaq\u201cLSHIFTpaq. Thus,\nthere exists a bitwise ReLU neural network \u03c8:t0,1uB\u00d1t0,1uBof width Band depth 1, such that\n\u03c8paq\u201cLSHIFTpa, bq.\nNeural network representation of RSHIFT A right-shift on t0,1uBis implemented by multi-\nplication by the following matrix:\nRdef.\u201c\u00bb\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u20130 0 0 0 \u00a8\u00a8\u00a80\n1 0 0 0 \u00a8\u00a8\u00a80\n0 1 0 0 \u00a8\u00a8\u00a80\n0 0 1 0 \u00a8\u00a8\u00a80\n...............0\n0 0\u00a8\u00a8\u00a8 0 1 0fi\nffiffiffiffiffiffiffiffiffiffiffifl\nLemma 21 (Right Shift) For any BPN,aPt0,1uBwe have IBReLUpRaq\u201cRSHIFTpaq. Thus,\nthere exists a bitwise ReLU neural network \u03c8:t0,1uB\u00d1t0,1uBof width Band depth 1, such that\n\u03c8paq\u201cRSHIFTpaq.\nA.6 Tropical Gates\nFor dynamic programming applications, we will rely on the following tropical gates.\nLemma 22 (Minimum and Maximum Function - (Petersen and Zech, 2024, Lemma 5.11))\nFor every nPN`withn\u011b2there exists a neural network \u03a6min\nn:Rn\u00d1Rwith depth rlog2pnqs, width\nat-most 3n, and at-most 16nnon-zero parameters: such that \u03a6min\nnpx1, . . . , x nq\u201cmin 1\u010fj\u010fnxj.\nSimilarly, there exists a neural network \u03a6max\nn:Rn\u00d1Rrealizing the maximum and satisfying the\nsame complexity bounds.\nLemma 23 (Median Function - (Hong and Kratsios, 2024, Lemma 7.2)) LetnPN`, the\nmedian function on 2n`1non-negative inputs can be implemented by a ReLU MLP of depth 11n`3,\nwidth 6n`3.\nLemma 24 (Majority Function) LetnPN`be odd and define the majority function, then Majn\ncan be implemented by a ReLU MLP of depth 11tn\n2u`4and width 6tn\n2u`3.\n33\n\n--- Page 34 ---\nKratsios, Zvigelsky, and Hart\nProof [Proof of Lemma 24] Consider the ReLU MLP \u03a6 : R\u00d1Rgiven by\n\u03a6pxqdef.\u201c \u00b42 ReLUpx\u00b41q`2 ReLUpxq\u201cp\u00b4 2,2qJReLU`\np1,1qx`p\u00b4 1,1q\u02d8\n.\nThen \u03a6p0q\u201c0, \u03a6p1\n2q\u201c1, and \u03a6p1q\u201c0.\nNext, observe that for any Boolean vector xP t0,1u2n`1, medianpxq P t 0,1\n2,1u; further,\nmedianpxq\u01050 only if t2n`1\n2ucomponents are non-zero. Thus,\nMaj2n`1\u201c\u03a6\u02ddmedian .\nConsequentially, Maj2n`1, Lemma 23 implies that Majncan be realized as a ReLU MLP of depth\n11n`4 and width max t2,6n`3u\u201c6n`3.\nA.7 Binary Arithmetic Operations\nOur first aim in the following is to show that floating-point arithmetic operations on bit-strings\ncan be represented exactly by neural networks (NNs). These NNs have conventional feedforward\narchitectures, constructed to map bit-string inputs, i.e. vectors of the form px1, . . . , x Bqconsisting\nof bits x1, . . . , x BPt0,1u, to bit-string outputs. We construct these NNs to perform floating-point\noperations, which are at the core of most (numerical) algorithms. We will rely on NN representa-\ntions of elementary logical operations on bits; using them, we will build representations of binary\ninteger arithmetic and then subsequently combine them to form NN representations of floating-point\narithmetic operation. Floating-point operations can be reduced to arithmetic operations on binary\nrepresentations of integers, of the form\n\u02d8B\u00b41\u00ff\nj\u201c0aj2j, a 0, . . . , a B\u00b41Pt0,1u.\nUnsigned integers Unsigned integers with Bbits are represented by paB\u00b41, . . . , a 0qPt0,1uB,\ncorresponding to the integers\u0159B\u00b41\nj\u201c0aj2j. These bit-strings can uniquely represent all integers\nt0, . . . , 2B\u00b41u. We define UINT Bdef.\u201c t0,1uB, and note the one-to-one correspondence,\nUINT B\u00bb#B\u00b41\u00ff\nj\u201c0aj2j:a0, . . . , a B\u00b41Pt0,1u+\n.\nAddition (mod 2B)Our aim in this paragraph is to bound the parameter count of a neural\nnetwork which exactly represents addition modulo 2B. The inputs to the network will be two\nelements a, bPUINT B, i.e. bit-representations of elements in ZXr0,2Bq. The output will be the\nbit-representation of a`bpmod 2Bq. To this end, we observe that addition of two numbers a, b\nencoded by Bbits can be written as B-fold iteration of the following bitwise operations:\n\u00a8\n\u02dda\nb\u02db\n\u201a\u00de\u00d1\u00a8\n\u02ddXORpa, bq\nLSHIFTpANDpa, bqq\u02db\n\u201a. (31)\nThe XORcomputes the addition without the carry, ANDcomputes the carry for each bit, the LSHIFT\nmoves the carry one bit up (which then, in the next iteration is going to be added).\n34\n\n--- Page 35 ---\nSystematic Neural Network Representations of Algorithms\nExample 2 For illustration, we consider a\u201c0001,b\u201c0111. In this case, a\u00bb1andb\u00bb7, so\nthata`b\u00bb8, i.e. a`bhas bit-representation 1000. In this case, using bitwise operations, we have\nthe following steps:\nStep 1: XORpa, bq\u201c0110,ANDpa, bq\u201c0001,LSHIFTpANDpa, bqq\u201c0010.\nWe now replace a\u00d00110,b\u00d00010.\nStep 2: XORpa, bq\u201c0100,ANDpa, bq\u201c0010,LSHIFTpANDpa, bqq\u201c0100.\nWe now replace a\u00d00100,b\u00d00100.\nStep 3: XORpa, bq\u201c0000,ANDpa, bq\u201c0100,LSHIFTpANDpa, bqq\u201c1000.\nWe now replace a\u00d00000,b\u00d01000.\nStep 4: XORpa, bq\u201c1000,ANDpa, bq\u201c0000,LSHIFTpANDpa, bqq\u201c0000.\nWe now replace a\u00d01000,b\u00d00000, and we terminate the algorithm.\nThe result is the value of a, i.e. a\u201c1000 the bit-representation of 8. Here we had B\u201c4bits\nand it took 4 steps.\nWe now show that bitwise addition can be represented efficiently by bitwise ReLU neural net-\nworks. To this end, we consider UINT B\u201ct0,1uBas a subset of RB, so that the application of a\nfeedforward neural network to a, bPUINT Bis well-defined.\nLemma 25 For any BPNanda, bPUINT B, we can represent (31) by a ReLU neural network\n\u03c8:RB\u02c6RB\u00d1RBof width 2Band depth 1.\nProof Recall the left-shift matrix Lfrom Lemma 20. We can represent the following mapping by\na bitwise ReLU neural network \u03c8of width 2 Band depth 1.\n\u00a8\n\u02dda\nb\u02db\n\u201a\u00de\u00d1\u00a8\n\u02ddReLUpa`bq\nReLUpa`b\u00b41q\u02db\n\u201a\u00de\u00d1\u00a8\n\u02ddReLUpa`bq\u00b42ReLUpa`b\u00b41q\nLReLUpa`b\u00b41q\u02db\n\u201a,\nBy Lemma 7, the first component of the output is XORpa, bq. By Lemma 5 and the definition of L,\nthe second component of the output is LSHIFTpANDpa, bqq.\nAs a consequence of the last lemma, it follows that there exists a neural network acting on bit-\nstring inputs, with width 2 Band depth at most B, which implements addition modulo 2B, denoted\nADDBpa, bq.\nLemma 26 For any BPN, we can represent bitwise addition ADDB:UINT B\u02c6UINT B\u00d1UINT Bby\na ReLU neural network \u03c8:RB\u02c6RB\u00d1RBof width 2Band depth B.\nProof ADDBis obtained by a B-fold composition of the operation (31). By Lemma 25, this opera-\ntion can be represented exactly by a ReLU neural network \u02dc\u03c8of width 2 Band depth 1. We define\n35\n\n--- Page 36 ---\nKratsios, Zvigelsky, and Hart\n\u03c8def.\u201c\u03c01\u02dc\u03c8\u02dd\u00a8\u00a8\u00a8\u02dd \u02dc\u03c8as the B-fold composition of \u02dc\u03c8, where \u03c01pa, bq\u201cais a projection onto the first\ncomponent. The claim now follows easily by (Petersen and Zech, 2024, Lemma 5.2)5.\nSigned integers We now consider bit-representations of signed integers in the range ZXp\u00b42B,2Bq.\nUsing a single bit \u03c1Pt0,1uto identify the sign as p\u00b41q\u03c1, we consider bit-strings a\u201cp\u03c1, aB\u00b41. . . a 0qP\nt0,1uB`1. These bit-strings are in correspondence with the integers in t\u00b4p2B\u00b41q, . . . , 2B\u00b41u, by\nassociating\np\u00b41q\u03c1B\u00b41\u00ff\nj\u201c0aj2j,\nwith each bit-string. This association is one-to-one except at 0, which possesses two representations,\np\u03c1,0, . . . , 0qfor both \u03c1\u201c0 and \u03c1\u201c1. We define the relevant set of bit-strings as INTBdef.\u201c t0,1uB`1.\nTwo\u2019s complement representation The signed integer representation above is very intuitive,\nbut not ideally suited to performing arithmetic operations. An alternative is based on the so-\ncalled \u201ctwo\u2019s complement\u201d. Similar to signed integers, we employ a representation in terms of\nB`1 bits: Non-negative integers in t0, . . . , 2B\u00b41uare still represented by bit-strings of the\nformp0, aB\u00b41, . . . , a 0q. However, in contrast to the signed integer representation introduced be-\nfore, negative integers are instead identified with additive inverses under 2B`1-modular arithmetic,\nand representing elements of t2B, . . . , 2B`1\u00b41u. To briefly summarize this, we consider an un-\nsigned integer a, represented by paB, aB\u00b41, . . . , a 0qwith aB\u201c0 and we note that in 2B`1-modular\narithmetic,\n\u00b4a\u201d\u00b4B\u00ff\nj\u201c0aj2j\u201d2B`1\u00b4B\u00ff\nj\u201c0aj2j\u201d20`B\u00ff\nj\u201c0p1\u00b4ajq2jpmod 2B`1q.\nThe expression on the right is represented by the bit-string sa\u201cADDB`1p0. . .01,NOTpaqq, and it\ncan be verified that ADDB`1pa,saq\u201c0. Thus, sais indeed a bit-string representation of \u00b4a, under\n2B`1-modular arithmetic. Let us now introduce\nCOMP B`1paqdef.\u201cADDB`1p0. . .01,NOTpaqq.\nWe then have the following result,\nLemma 27 COMP B`1can be represented by a neural network of width 2B`2and depth B`2.\nProof By Lemma 26, ADDB`1can be represented by a ReLU neural network of width 2 B`2\nand depth B`1. By Lemma 4, NOTcan be represented by a ReLU neural network of width\nB`1 and depth 1. By (Petersen and Zech, 2024, Lemma 5.2), the composition COMP B`1paq \u201c\nADDB`1p0. . .01,NOTpaqqcan be represented by a neural network of width 2 B`2 and depth B`2.\nThe positive integers are represented by strings of the form p0, aB\u00b41, . . . , a 0q. Applying COMP,\nsuch a string gets mapped to a string of the form p1,saB\u00b41, . . . ,sa0q, where at least one of sa0, . . . ,saB\u00b41\n5. If the user would rather have a feedforward emulator, rather than an MLP emulator, then there is no need to\napply (Petersen and Zech, 2024, Lemma 5.2).\n36\n\n--- Page 37 ---\nSystematic Neural Network Representations of Algorithms\nis non-zero. Since COMP B`1is an implementation of a\u00de\u00d1\u00b4aunder 2B`1arithmetic, applying this\nmapping twice gives the identity, i.e. COMP B`1\u02ddCOMP B`1\u201cIDENTITY (henceforth assumed to be\nthe neural network defined in (Petersen and Zech, 2024, Lemma 5.1)).\nTo summarize, the unsigned integer representation from before ( INTB), and the two\u2019s comple-\nment representation here, both involve bit-strings of length B`1. However, the interpretation\nof these bit-strings is very different. To make this distinction apparent in our notation, we define\nINTBdef.\u201c t0,1uB`1.\nWe can now define a bijection between the signed integer representation INTBand the two\u2019s\ncomplement representation INTB, by defining \u03a6INTB\u00d1INTB:INTB\u00d1INTB, as\n\u03a6INTB\u00d1INTBpaqdef.\u201c#\na, ifa\u201cp0, aB\u00b41, . . . , a 0q,\nCOMP B`1p0, aB\u00b41. . . a 0q,ifa\u201cp1, aB\u00b41, . . . , a 0q.(32)\nThe following is immediate from our previous discussion:\nLemma 28 Leta\u201cp\u03c1, aB\u00b41, . . . , a 0qPINTBbe a signed integer bit-string, where \u03c1represents the\nsignp\u00b41q\u03c1. Denote \u02c6adef.\u201c paB\u00b41, . . . , a 0q, i.e. awith the sign-bit \u03c1removed. Then\n\u03a6INTB\u00d1INTBpaq\u201cADDB`1pANDpNOTp\u03c1q,0\u02c6aq,ANDp\u03c1,COMP B`1p0\u02c6aqqq.\nconverts from signed integers to two\u2019s complement. A careful analysis will reveal that \u03a6INTB\u00d1INTB\nis in fact an involution; and so, the inverse is computed in the same way:\n\u03a6\u00b41\nINTB\u00d1INTBpaq\u201cADDB`1pANDpNOTp\u03c1q,0\u02c6aq,ANDp\u03c1,COMP B`1p0\u02c6aqqq.\nNote that in the neural network, \u03c1is either the 1Bor 0Bvector. Thus, conversion between signed\ninteger INTBand two\u2019s complement INTBrepresentations can be represented by a combination of\nmodular addition, logic gates and COMP B`1. We also note that\nLemma 29 \u03a6INTB\u00d1INTBand\u03a6\u00b41\nINTB\u00d1INTBcan be represented by a neural network of width 7B`2\nand depth 2B`4.\nProof Let\u03c0\u03c1,\u02c6\u03c0be projections, defined by \u03c0\u03c1paq\u201c\u03c1, \u02c6\u03c0paq\u201c0\u02c6a. We can write \u03a6INTB\u00d1INTBas a\ncomposition involving the following sequence of maps:\n\u00a8\n\u02da\u02da\u02da\u02da\u02da\u02ddNOT\u02dd\u03c0\u03c1\nIDENTITY\u02dd\u02c6\u03c0\nIDENTITY\u02dd\u03c0\u03c1\nCOMP B`1\u02dd\u02c6\u03c0\u02db\n\u2039\u2039\u2039\u2039\u2039\u201a\u00d1\u00a8\n\u02ddAND\nAND\u02db\n\u201a\u00d1ADDB`1.\nBy Lemma 4, NOT, acting on bit-strings of length Bcan be represented by a neural network of depth\n1, and of width B. By (Petersen and Zech, 2024, Lemma 5.1), IDENTITY can be represented by\na neural network of depth 1 and width 2 B. By Lemma 27, COMP B`1is representable by a neural\nnetwork of width 2 B`2 and depth B`2. Parallelizing these networks, it follows that the first\nmapping in the display above can be represented by a neural network of width B`2B`2B`p2B`\n2q\u201c5B`2 and depth B`2.\n37\n\n--- Page 38 ---\nKratsios, Zvigelsky, and Hart\nBy Lemma 5, AND, acting on bit-strings of length B`1, can be represented by a neural network\nof width B`1 and depth 1. By parallelization, pAND,ANDqcan thus be represented by a network of\nwidth 2 B`2 and depth 1.\nBy Lemma 26 ADDB`1can be represented by a neural network of width 2 B`2 and depth B`1.\nComposing these network representations, we conclude that \u03a6INTB\u00d1INTBcan be represented by a\nneural network of width max p7B`2,2B`2,2B`2q\u201c7B`2, and depthpB`2q`1`pB`1q\u201c2B`4.\nA similar argument applies to \u03a6\u00b41\nINTB\u00d1INTB, leading to the claim.\nExact Integer Addition Exact integer addition on signed integer bit-strings a, bPINTBgen-\nerally requires outputs with B`2 bits, i.e. elements in INTB`1. Indeed, if a, bare bit-strings\nrepresenting elements in ZXr\u00b42B,2B\u00b41s, then a`bis inZXr\u00b42B`1,2B`1\u00b41sand hence can\nbe represented by a bit-string in INTB`1. Define EMBED B:INTB\u00d1INTB`1by\nEMBED Bpaq\u201cEMBEDpaB. . . a 0qdef.\u201c#\n0aB. . . a 0,ifaB\u201c0,\n1aB. . . a 0,ifaB\u201c1.\nIn particular, note how this embedding preserves additive inverses, where EMBED Bpaq\u201caB`1aBaB\u00b41. . . a 0\nis obtained by replicating the first bit. So,\nEMBED B\u201c\u00a8\n\u02dd1 0 0 . . .0\nIB\u02db\n\u201a.\nWe can realize exact integer addition with a neural network:\nLemma 30 (Exact integer addition) For any BPN, there exists a neural network \u03c8:RB`1\u02c6\nRB`1\u00d1RB`2of width at-most 2B`4and depth B`2, such that \u03c8pa, bqis the (exact) signed\ntwo\u2019s complement integer bit-string of a`b, represented by a bit-string in INTB`1.\nProof We can write the mapping \u03c8in terms of the following sequence of maps:\n\u00a8\n\u02dda\nb\u02db\n\u201a\u00de\u00d1\u00a8\n\u02ddEMBED B\nEMBED B\u02db\n\u201a\u00de\u00d1ADDB`2\u00de\u00d1a`b.\nBy Lemma 26, ADDB`2can be represented by a neural network of width 2 B`4 and depth B`2.\nComposing these networks, we obtain a neural network representation of exact integer addition\nINTB\u02c6INTB\u00d1INTB`1, with a neural network of width 2 B`4 and depth B`2.\nExact Integer Multiplication The multiplication of a, bPUINT B, with representations a\u201c\npaB\u00b41. . . a 0qandb\u201c pbB\u00b41. . . b 0q, has the following properties: Firstly, observe that if a\u201c\np0. . .0a0q, then we either multiply by 0 (if a0\u201c0) or 1 (if a0\u201c1), and hence\nMULTp0. . .0a0, bq\u201cp ANDpa0, bB\u00b41q, . . . , ANDpa0, b0qq.\n38\n\n--- Page 39 ---\nSystematic Neural Network Representations of Algorithms\nOn the other hand, if a\u201cp0. . .010q(representing the integer 2), then\nMULTp0. . .010, bq\u201cLSHIFTpbq.\nIn general, we then have\nMULTpaB\u00b41. . . a 0, bq\u201cADDpMULTpaB\u00b41. . . a 10, bq,MULTp0. . .0a0, bqq,\nand\nMULTpxB. . . x 20, yq\u201cMULTp0. . .010,MULTpxB. . . x 2, yqq\u201c LSHIFTpMULTpxB. . . x 2, yqq.\nso we have the recursion,\nMULTpxB. . . x 1, yq\u201cADDpLSHIFTpMULTpRSHIFTpxq, yqq,MULTp0. . .0x1, yqq\nThis should allow us to compute the required depth and width to represent MULT; ifMULT 1px1, yqdef.\u201c\nMULTp0. . .0x1, yq,MULT 2px2x1, yqdef.\u201cMULTp0. . .0x2x1, yqetc., then we know that we can represent\nMULT 1px1, yqby a ReLU neural network of width 2 Band depth 1. The recursion is then\nMULT BpxB. . . x 1, yq\u201cADDpLSHIFTpMULT B\u00b41pRSHIFTpxB. . . x 1q, yqq,MULT 1px1, yqq\nThis implies that\nwidthpMULT Bq\u010fmaxtwidthpADDpLSHIFTp\u00a8q,\u00a8qq,widthpMULT B\u00b41q`widthpMULT 1qu\n\u201cmaxt2B,widthpMULT B\u00b41q`2Bu\n\u201cwidthpMULT B\u00b41q`2B.\nBy recursion on B, it follows that width pMULT Bq\u010fB2`1.\nSimilarly, for the depth, we have\ndepthpMULT Bq\u010fdepthpADDpLSHIFTp\u00a8q,\u00a8qq` maxtdepthpMULT B\u00b41q,depthpMULT 1qu\n\u201cB`depthpMULT B\u00b41q.\nBy recursion on B, it follows that depth pMULT Bq\u010fB2.Let us summarize the above discussion in\nthe following lemma:\nLemma 31 (Bitwise Multiplication) For any BPN, we can represent bitwise multiplication\nMULT :UINT B\u02c6UINT B\u00d1UINT Bby a bitwise ReLU neural network \u03c8MULT:t0,1uB\u02c6t0,1uB\u00d1t0,1uB\nof width\u010fB2`1and depth\u010fB2.\nA.8 Modular Arithmetic in Rq\nWe will construct neural networks that compute modular addition and multiplication.6\nGiven a, bPRq, we show how to compute a`Rqbpmod 2q`1q; it follows that it is sufficient\nto compute 2\u00b4qp2qa`Rq2qbq pmod 2q`1q. Towards this goal, we will embed a, binto INT2q`1, as\nADD2q`2as defined in Lemma 26 performs addition modulo 2q`1on members of INT2q`2. Using the\nfact that bpmod cq\u201c1\napabpmod acqq,we divide the result by 2q.\n6. We could have also defined addition and multiplication in Rq(and the equivalent neural networks) to be saturated .\nWe leave it as an exercise for the reader to verify that it is possible.\n39\n\n--- Page 40 ---\nKratsios, Zvigelsky, and Hart\nLemma 32 (Modular Addition in Rq)FixqPN`. Then, there exists a ReLU MLP \u03a6`,q:\nRq\u02c6Rq\u00d1Rqof depth and width Opqqsuch that for each a, bPRq,\n\u03a6\u02c6,qpa, bq\u201c\u00b4\nt|y|uloomoon\nn:integer part,|y|\u00b4 t|y|uloooomoooon\nr:remainder, Iy\u011b0loomoon\ns:sign\u00afJ\nwhere y\u201ca`Rqbpmod 2q`1q.\nProof Based on the discussion above, we can summarize our intended computation as a sequence\nof maps\u00a8\n\u02dda\nb\u02db\n\u201a\u00de\u00d1\u00a8\n\u02dd\u03a6INT2q`1\u00d1INT2q`1pRpaqq\n\u03a6INT2q`1\u00d1INT2q`1pRpbqq\u02db\n\u201a\u00de\u00d1\u00b4\nL`\nADD2q`2p\u00a8,\u00a8q\u02d8\u00af\n,\nwhere R,Lare right and left shift operators that move the sign bit to the leftmost bit and\nrightmost bit, respectively. Now recall that for any BPN`, we have that depthpADDBq \u010fB,\ndepthp\u03a6INTB\u00d1INTBq\u010f2B`4,widthpADDBq\u010f2B, and widthp\u03a6INTB\u00d1INTBq\u010f7B`2 by Lemmas 26\nand 29. Lemmas 5.2 and 5.3 of Petersen and Zech (2024) imply the existence of a ReLU MLP\n\u03a6`,qpa, bq\u201cLpADD2q`2p\u03a6INT2q`1\u00d1INT2q`1pRpaqq,\u03a6INT2q`1\u00d1INT2q`1pRpbqqqq\nsuch that depthp\u03a6`,qq\u010f6q`8 and widthp\u03a6`,qq\u010f56`36. Hence, the claim holds.\nGiven a, bPRq, we show how to compute a\u02c6Rqbpmod 2q`1q. Notice that 2\u00b4q\u010f|a\u02c6Rqb|\u010322q`2.\nThus, we only need 4 q`2\u201c2p2q`1qbits to capture multiplication. Since a, bPRq, we have\na\u201cp\u00b4 1qsaq\u00ff\ni\u201c\u00b4q\u03b1i2i,\nb\u201cp\u00b4 1qsbq\u00ff\ni\u201c\u00b4q\u03b2i2i\nwhere \u03b1i, \u03b2iPt0,1ufor all\u00b4q\u010fi\u010fq. Thus, we can view a\u02c6Rqbas\np\u00b41qsa`sb2\u00b4q\u02dc2q\u00ff\ni\u201c0\u03b1i2i\u00b8\n\u02c6Rq2\u00b4q\u02dc2q\u00ff\ni\u201c0\u03b2i2i\u00b8\n.\nNow for any q, let\nA4q`2\u201c\u00a8\n\u02dd02q`1,0p2q`1,1q\nI2q`1,0p2q`1,1q\u02db\n\u201a.\nGiven a\u201cp\u03b1iqq\ni\u201c0\u2018p\u03b1iq\u00b41\ni\u201c\u00b4q\u2018asandb\u201cp\u03b2iqq\ni\u201c0\u2018p\u03b2iq\u00b41\ni\u201c\u00b4q\u2018as, we can compute the sign bit\nwith ADD1psa, sbq. For the integer and fractional components, first compute\nx\u201cMULT 4q`2pA4q`2paq, A4q`2pbqq\n40\n\n--- Page 41 ---\nSystematic Neural Network Representations of Algorithms\nand notice that the integer component is contained in the leftmost 2 q`2 bits and the fractional\ncomponent is contained in the rightmost 2 qbits. We can round xto have qbits in the fractional\ncomponent by rounding up. That is, compute\ng\u201cADD4q`2px,MULTp1, P3q`3pxqqq,\nwhere P3q`3is a 4 q`2 by 4 q`2 matrix with 1 at p3q`3,3q`3qand 0 elsewhere. We can represent\ngasp\u03b3iq2q`2\ni\u201c0\u2018p\u03b3iq\u00b41\ni\u201c\u00b42qNotice that\n2q`2\u00ff\ni\u201c0\u03b3i2i\u201cq\u00ff\ni\u201c0\u03b3i2i`2q`2\u00ff\ni\u201cq`1\u03b3i2i\u201dq\u00ff\ni\u201c0\u03b3i2ipmod 2q`1q.\nNow define\nT\u201c\u00b4\n0p2q`1,q`1qIq`10pq`1,qq\u00af\nto be the matrix that selects the bits \u03b3ifor\u00b4q\u010fi\u010fq`1. Then Tpgqis the computation of a\u02c6Rqb.\nLemma 33 (Modular Multiplication in Rq)FixqPN`. Then, there exists a ReLU MLP\n\u03a6\u02c6,q:Rq\u02c6Rq\u00d1Rqof depth and width Opq2qsuch that for each a, bPRq,\n\u03a6\u02c6,qpa, bq\u201c\u00b4\nt|y|uloomoon\nn:integer part,|y|\u00b4 t|y|uloooomoooon\nr:remainder, Iy\u011b0loomoon\ns:sign\u00afJ\nwhere y\u201ca\u02c6Rqbpmod 2q`1q.\nProof Based on the discussion above, we can summarize our intended computation as a sequence\nof maps\n\u00a8\n\u02dda\nb\u02db\n\u201a\u00de\u00d1\u00a8\n\u02ddMULT 4q`2pA4q`2paq, A4q`2pbqq\nADD1p\u03c02q`2paq, \u03c02q`2pbqq\u02db\n\u201a\u00de\u00d1\u00a8\n\u02ddADD4q`2p\u00a8, P3q`3p\u00a8qq\n\u00a8\u02db\n\u201a\u00de\u00d1\u00a8\n\u02ddTp\u00a8q\n\u00a8\u02db\n\u201a,\nwhere \u03c02q`2is the 1 by 2 q`2 matrix that projects the last (sign) bit of vectors in Rq. Now recall\nthat for any BPN`, we have that depthpMULT Bq\u010fB2,depthpADDBq\u010fB,widthpMULT Bq\u010fB2`1,\nandwidthpADDBq\u010f2Bby Lemmas 26 and 31. Lemmas 5.2 and 5.3 of Petersen and Zech (2024)\nimply the existence of a ReLU MLP\n\u03a6\u02c6,q\u201c\u00a8\n\u02ddTpADD4q`2pMULT 4q`2pA4q`2paq, A4q`2pbqq, PpMULT 4q`2pA4q`2paq, A4q`2pbqqqqq\nADD1p\u03c02q`2paq, \u03c02q`2pbqq\u02db\n\u201a\nsuch that depthp\u03a6\u02c6,qq \u010f p 4q`2qp4q`3qandwidthp\u03a6\u02c6,qq \u010f 2p4q`2q2`6. Hence, the claim\nholds.\n41\n\n--- Page 42 ---\nKratsios, Zvigelsky, and Hart\nB. Proof of Theorem 2\nProof [Proof of Theorem 2] Enumerate Rd\nq\u201ctxnu2dq\nn\u201c0andRd\nq\u201ctynu2dq\nn\u201c0. For each nP\u201c\n22dq\u2030\n, let\n\u03a6ndef.\u201c\u03a6xn,qbe the corresponding ReLU MLP constructed in Lemma 14 (having width at-most 4 d,\ndepth 4, and 18 d`5 non-zero parameter satisfying) and satisfying\n\u03a6npxq\u201cIx\u201cxnp@xPRd\nqq. (33)\nApplying the parallelization lemma, see (Petersen and Zech, 2024, Lemma 5.3), we deduce that the\nexists a ReLU MLP \u03a6 Enc:Rd\u00d1R2dqsatisfying\n\u03a6pxq\u201c\u00b4\n\u03a6npxq\u00af2qd\nn\u201c0\u201c\u00b4\nIx\u201cxn\u00af2qd\nn\u201c0. (34)\nfor each xPRd. Moreover, \u03a6 has width at-most 2 22dqp4dq \u201cd23`2dq, depth 4, and less than\nd3222`2qdnon-zero parameters. Consider the D\u02c622dq-matrix \u03b2fwith entries in Rqwhose nth\ncolumnp\u03b2fqn, for each nP\u201c\n22dq\u2030\n, is given by\np\u03b2fqndef.\u201cfpxnq.\nThen, by the right-hand side of (34) we have: for each each xnPRd\nq\n\u03b2f\u03a6Encpxnq\u201cp\u03b2fqn\u201cfpxnq.\nThus, \u03b2f\u03a6Encsatisfies our claim with no more than d3222`qd`2pd`Dqpnon-zero parameters.\nC. Proof of Applications and Corollaries\nProof [Proof of Corollary 2] Let X\u00a8def.\u201c pXkqB`m\nk\u201cB`1be i.i.d. Bernoulli random variables with success\nprobability p\u01051\n2and let Abe aGlgccircuit, for which the p-randomized Glog-circuitpA, X\u00a8q\ncomputes fwith probability p, as defined in (8). Then, the proof of (Adleman, 1978, Theorem 1)\ngiven in (Jukna, 2012a, page 15) consider the \u201cprobabilistic circuit\u201d AKwhich computes\nCptpAKqpxqdef.\u201cMaj8BK`1`\nCptpApx, X\u00a8q, . . . , CptpAqpx, X\u00a8q\u02d8\n(35)\n(we allow for one extra copy, but the union bound in (Jukna, 2012a, page 15) was already vacuous\nat 8nKcopies of A) is deterministic due to (Jukna, 2012a, Lemma 1.5). Therefore, Lemma 24\nimplies that there is a ReLU MLP of 44 BK`4 and width 33 BK`3 implementing Maj8BK`1on\nt0,1u8BK`1.\nSince Ais a randomized circuit with Kcomputation nodes, each with fan-in Band gate in\nGlgc. Then lemmata 2, 3, 4, 5, 6, 7, and 8 imply that the largest ReLU MLP implementation of\nthese basic logic gates in Glgcwith fan-in is the one used to represent EQUAL in Lemma 2 which\nhas depth 4, width at-most rB\n2sand at-most 13 rB\n2snon-zero parameters and the widest is the one\nused to represent NOT in Lemma 4 which has width B; thus, no ReLU MLP representation of any\nof these logic gates has depth more than 4, width more than B, and more than 13 rB\n2snon-zero\nparameters. Thus, there exists a ReLU MLP with no more than depth 4\u2206 width wB, and K13rB\n2s\nnon-zero parameters implementing Cpt pAq.\n42\n\n--- Page 43 ---\nSystematic Neural Network Representations of Algorithms\nBy the Parallelization Lemma in (Petersen and Zech, 2024, Lemma 5.3), there is a ReLU MLP\nimplementing\nx\u00de\u00d1Maj8BK`1`\nCptpApx, X\u00a8q, . . . , CptpAqpx, X\u00a8q\u02d8\nwith width at-most 2 p8BK`1q, depth 4\u2206, and with no more than 26 KrB\n2snon-zero parameters.\nConsequentially, the \u201cderandomized representation\u201d in (35) implies that Cpt pAKqcan be im-\nplemented by a ReLU MLP of width at-most 44 BK`4``2p8BK`1q \u201c 60BK`6, depth\n33BK`3`4\u2206, and with no more than 26 KrB\n2snon-zero parameters.\nProof [Proof of Corollary 3]\nStep 1 - Computation at any single intermediate time tPrTs:LetMbe a turning time\nwith time-bound T. For each tPrTs, (Pippenger and Fischer, 1979, Theorem 4) guarantees that\nthere is a Glgcdef.\u201c tAND 2,OR2,NOTu-circuit Atsimulating M; i.e.\nCptpAtqpxq\u201cMpxqt (36)\nfor all xPt0,1uB. Moreover, Ahas depth Optqand computation units Optlogptqq; where Oignores\nthe dependence on B.\nNow, similarly to the middle of the proof of Corollary 2, by lemmata 4, 5, and 6 imply that\nall the largest ReLU MLP implementation of these basic logic gates in Glgccan be represented as\nReLU MLPs of depth 4, width width at-most B, with at-most 13 rB\n2snon-zero parameters. Thus,\nthere exists a ReLU neural network, with depth O`\nt\u02d8\nand at-most OpBtlogptqqcomputation units,\nimplementing Aont0,1uB; i.e.\nCptpAqpxq\u201c\u02c6ftpxq (37)\nfor all xPt0,1uB. Combining (36) and (37) implies that\n\u02c6ftpxq\u201cMpxqt (38)\nfor each xP\u03a3.\nStep 2 - Identification of Time Step: By Lemma 14, for each tPrTsthere exists a ReLU MLP\n\u03a6t,q:R\u00d1Rwith width at-most 4, depth 4, and 23 non-zero parameter satisfying\n\u03a6t,qpsq\u201cIs\u201ct\nfor each sPR1\nq.\nStep 3 - Aggregating Steps 1and 2:LetPx:RB`1\u00d1RBandPt:RB`1\u00d1Rbe the\nprojection matrices, respectively, mapping any px, tq PRB`1toxand anypx, tq PRB`1tot.\nApplying the parallelization lemma in (Petersen and Zech, 2024, Lemma 5.3), there exists a ReLU\nMLP \u02dc\u03a6 :RB`1\u00d1R2B\nqsatisfying\n\u02dc\u03a6pxq\u201c\u00b4\n\u02c6ftpPxxq,T\u00e0\nt\u201c0\u03a6t,qpPtxq\u00af\n(39)\nfor each xPRB`1\nq; moreover, \u02dc\u03a6 has at-most OpT\u02c6TlogpTqqnon-zero parameters (still suppressing\nany dependence on B). Now, applying Lemma 31 we deduce that there exists a ReLU MLP\n43\n\n--- Page 44 ---\nKratsios, Zvigelsky, and Hart\n\u03c8MULT :t0,1uB\u02c6t0,1uB\u00d1t0,1uBof width B2and depth B2implementing bitwise multiplication.\nThus, for each px, sqPt0,1uB\u02c6R1\nq\n\u03a6M:Tpx, sqdef.\u201c1J\nT`1\u03c8MULT\u02dd\u02c6\n\u02c6ftpPxxq,T\u00e0\nt\u201c0\u03a6t,qpPtxq\u02d9\n\u201cT\u00ff\nt\u201c0\u02c6ftpxqIs\u201ct\u201cMpxqs (40)\nas desired; were we recall that, 1T`1PR1`Tdenotes the vector with all entries equal to 1. A brief\nverification shows that the number of non-zero parameters of \u03a6 M:Tis still OpT2logpTqq, since we\nsurprise the dependence on B.\nProof [Proof of Lemma 1] Let xPr0,1sdandqPN`. We have\n\u203a\u203afpxq\u00b4CptpAq\u02dd\u03c0d\nqpxq\u203a\u203a\u010f}\u03c0D\nq\u02ddf\u02dd\u03c0d\nqpxq\u00b4f\u02dd\u03c0d\nqpxq}8`}fpxq\u00b4f\u02dd\u03c0d\nqpxq}8\n`}\u03c0D\nq\u02ddf\u02dd\u03c0d\nqpxq\u00b4CptpAq\u02dd\u03c0d\nqpxq}8\n\u010f2\u00b4p`1`}fpxq\u00b4f\u02dd\u03c0d\nqpxq}8\n`}\u03c0D\nq\u02ddf\u02dd\u03c0d\nqpxq\u00b4CptpAq\u02dd\u03c0d\nqpxq}8.\nThis completes our proof.\nProof [Proof of Proposition 5] Let Aqbe a ReLU MLP such that with Cpt pAqq:Rd\nq\u00d1RD\nq, to be\ndetermined retroactively. By the inequality in Lemma 1, we have\nsup\nxPr0,1sd\u203a\u203afpxq\u00b4CptpAq\u02dd\u03c0d\nqpxq\u203a\u203a\u010f2\u00b4p`1`\u03c9p2\u00b4p`1q`} \u00affqpxq\u00b4CptpAqq\u02dd\u03c0d\nqpxq}8looooooooooooooooomooooooooooooooooon\n(II). (41)\nSince \u00affq:Rd\nq\u00d1RD\nqthen Theorem 2 implies that there is a ReLU MLP Aqcomputing \u00affq;\nwhence (II)\u201c0.\nD. Pseudocode Yielding The Universal Approximator of Theorem 2\nThis section contains a brief implementation of the universal approximator in Theorem 2. For any\na\u01050 and bPRd, we write\ngpx;a, bqdef.\u201c\u015bd\nj\u201c1`\nReLUpapxj\u00b4bjq\u00b43\n2q\u00b4ReLUpapxj\u00b4bjq\u00b41\n2q\u00b4ReLUpapxj\u00b4bjq`1\n2q`ReLUpapxj\u00b4bjq`3\n2q\u02d8\n.\nRecall that the multiplication operation may be computed by a ReLU MLP (see Lemma 31). Having\ndefined this basic helper function, Algorithm 1 effectively implements the universal approximator\nof Theorem 2 when the inputs data is X\u201cRd\nqandY\u201cpfpqqqqPRdq. In this way, it is an algorithmic\nversion of universal approximation theorems which rely on finitely many aptly computed sample\nvalues from the target function; e.g. Franco and Brugiapaglia (2024); Hong and Kratsios (2024);\nNeuman et al. (2025).\n44\n\n--- Page 45 ---\nSystematic Neural Network Representations of Algorithms\nAlgorithm 1: Universal Circuit\nRequire: XPRN\u02c6d,YPRN\u02c6D\nEnsure: Approximator \u02c6f:Rd\u00d1RD\n1:a\u00d01\nmaxtminn\u2030m}Xn\u00b4Xm}8{2,2q`1u\n2:\u02c6fpxq\u00d00\n3:forn\u201c1, . . . , N do\n4: \u02c6fpxq\u00d0 \u02c6fpxq`Yn\u00a8gpx;a, X nq\n5:end for\n6:return \u02c6fpxq\nx0.0\n0.2\n0.4\n0.6\n0.8\n1.0y\n0.00.20.40.60.81.0F(x,y)\n1.0\n0.5\n0.00.51.01.5Approximator of f(x,y)=sin(2x)ey\nFigure 5: Construction and visualization of the universal approximator. Left: pseudocode for\ncomputing the universal approximator in Theorem 1, and Right: typical graph of that function\nimplemented by this construction.\nE. Quoted/Known ReLU MLP Constructions\nThis appendix collects useful ReLU MLP constructions that are available in the literature.\nThe following result is quoted from (Petersen and Zech, 2024, Lemma 5.3).\nLemma 34 (Parallelization) LetmPNandp\u03a6iqm\ni\u201c1be neural networks with architectures p\u03c3ReLU;di\n0, . . . , di\nLi`1q,\nrespectively. Then the neural network p\u03a61, . . . , \u03a6mqsatisfies\np\u03a61, . . . , \u03a6mqpxq\u201cp\u03a61px1q, . . . , \u03a6mpxmqqfor all xPR\u0159m\nj\u201c1dj\n0.\nMoreover, with Lmaxdef.\u201cmax j\u010fmLj, it holds that\nwidthpp\u03a61, . . . , \u03a6mqq\u010f2m\u00ff\nj\u201c1widthp\u03a6jq, (5.1.5a)\ndepthpp\u03a61, . . . , \u03a6mqq\u201cmax\nj\u010fmdepthp\u03a6jq, (5.1.5b)\nsizepp\u03a61, . . . , \u03a6mqq\u010f2m\u00ff\nj\u201c1sizep\u03a6jq`2m\u00ff\nj\u201c1pLmax\u00b4Ljqdj\nLj`1. (5.1.5c)\nThe following result is quoted from (Hong and Kratsios, 2024, Proposition 7.2).\nProposition 3 (Optimal Memorization by Two-Hidden-Layer MLPs) LetM, NPN`. For\nany set of MN samples Sdef.\u201c pxi, yiqMN\ni\u201c1\u010eR2(where x1\u0103x2\u0103\u00a8\u00a8\u00a8\u0103 xMN), there exists a ReLU\nMLP \u03a6with widthvec\u201crM,4N\u00b42sthat can memorize this sample set; i.e.\n\u03a6Spxiq\u201cyi fori\u201c1, . . . , MN.\nFurthermore, \u03a6is linear on the intervals rxi, xi`1sfori\u201c1,2,\u00a8\u00a8\u00a8, MN\u00b41, and it is constant on\neach of the segment p\u00b48, x1sandrxMN,8q. The number of nonzero parameters in \u03a6is at-most\n2MN`2M`8N\u00b44.\n45\n\n--- Page 46 ---\nKratsios, Zvigelsky, and Hart\nReferences\nAhmed Abdeljawad and Thomas Dittrich. Weighted Sobolev Approximation Rates for Neural\nNetworks on Unbounded Domains. arXiv preprint arXiv:2411.04108 , 2024.\nBen Adcock, Simone Brugiapaglia, and Clayton G Webster. Compressed sensing approaches for\npolynomial approximation of high-dimensional functions. In Compressed Sensing and its Appli-\ncations: Second International MATHEON Conference 2015 , pages 93\u2013124. Springer, 2018.\nBen Adcock, Simone Brugiapaglia, and Clayton G Webster. Sparse polynomial approximation of\nhigh-dimensional functions , volume 25. SIAM, 2022.\nBen Adcock, Simone Brugiapaglia, Nick Dexter, and Sebastian Moraga. Learning smooth functions\nin high dimensions: from sparse polynomials to deep neural networks, 2024. URL https://\narxiv.org/abs/2404.03761 .\nBen Adcock, Simone Brugiapaglia, Nick Dexter, and Sebastian Moraga. Near-optimal learning\nof Banach-valued, high-dimensional functions via deep neural networks. Neural Networks , 181:\n106761, 2025.\nLeonard Adleman. Two theorems on random polynomial time. In 19th Annual Symposium on\nFoundations of Computer Science (sfcs 1978) , pages 75\u201383. IEEE Computer Society, 1978.\nJanice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large Language\nModels for Mathematical Reasoning: Progresses and Challenges. In The 18th Conference of the\nEuropean Chapter of the Association for Computational Linguistics , page 225, 2024.\nAndrew L Allan, Chong Liu, and David J Pr\u00a8 omel. A c` adl` ag rough path foundation for robust\nfinance. Finance and Stochastics , 28(1):215\u2013257, 2024.\nV. I. Arnold. On functions of three variables. Amer. Math. Soc. Transl. (2) , 28:51\u201354, 1963.\nVladimir Igorevich Arnol\u2019d. On the representation of continuous functions of three variables by\nsuperpositions of continuous functions of two variables. Matematicheskii Sbornik , 90(1):3\u201374,\n1959.\nMukund Balasubramanian and Eric L Schwartz. The isomap algorithm and topological stability.\nScience , 295(5552):7\u20137, 2002.\nAndrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function.\nIEEE Transactions on Information theory , 39(3):930\u2013945, 1993.\nPeter Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear vc dimension bounds for piecewise\npolynomial networks. Advances in neural information processing systems , 11, 1998.\nPeter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension\nand pseudodimension bounds for piecewise linear neural networks. Journal of Machine Learning\nResearch , 20(63):1\u201317, 2019.\n46\n\n--- Page 47 ---\nSystematic Neural Network Representations of Algorithms\nChristian Beck, Lukas Gonon, and Arnulf Jentzen. Overcoming the curse of dimensionality in\nthe numerical approximation of high-dimensional semilinear elliptic partial differential equations.\nPartial Differ. Equ. Appl. , 5(6):Paper No. 31, 47, 2024. ISSN 2662-2963,2662-2971. doi: 10.1007/\ns42985-024-00272-4. URL https://doi.org/10.1007/s42985-024-00272-4 .\nSilvia Beddar-Wiesing, Giuseppe Alessio D\u2019Inverno, Caterina Graziani, Veronica Lachi, Alice\nMoallemy-Oureh, Franco Scarselli, and Josephine Maria Thomas. Weisfeiler\u2013lehman goes dy-\nnamic: An analysis of the expressive power of graph neural networks for attributed and dynamic\ngraphs. Neural Networks , 173:106213, 2024.\nDaniel Bertschinger, Christoph Hertrich, Paul Jungeblut, Tillmann Miltzow, and Simon Weber.\nTraining fully connected neural networks is r-complete. Advances in Neural Information Process-\ning Systems , 36:36222\u201336237, 2023.\nOleg V. Besov. On some families of functional spaces: Imbedding and extension theorems. In\nDoklady Akademii Nauk SSSR , volume 126, pages 1163\u20131165, 1959.\nAnselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Learnability and\nthe Vapnik-Chervonenkis dimension. Journal of the ACM (JACM) , 36(4):929\u2013965, 1989.\nHolger Boche, Adalbert Fono, and Gitta Kutyniok. Limitations of deep learning for inverse problems\non digital hardware. IEEE Transactions on Information Theory , 2023.\nHolger Boche, Adalbert Fono, and Gitta Kutyniok. Inverse problems are solvable on real number\nsignal processing hardware. Applied and Computational Harmonic Analysis , 74:101719, 2025.\nB\u00b4 ela Bollob\u00b4 as. Random graphs , volume 73 of Cambridge Studies in Advanced Mathematics . Cam-\nbridge University Press, Cambridge, second edition, 2001. ISBN 0-521-80920-7; 0-521-79722-5.\ndoi: 10.1017/CBO9780511814068. URL https://doi.org/10.1017/CBO9780511814068 .\nDigvijay Boob, Santanu S Dey, and Guanghui Lan. Complexity of training relu neural network.\nDiscrete Optimization , 44:100620, 2022.\nOlivier Bournez, Valentin Dardilhac, and Johanne Cohen. On the delta-decidability of decision\nproblems for neural network questions. In Continuity, Computability, Constructivity. From Logic\nto Algorithms. CCC\u201923 , 2023.\nOlivier Bournez, Johanne Cohen, and Adrian Wurm. A universal uniform approximation theorem\nfor neural networks. In Proceedings of the International Symposium on Mathematical Foundations\nof Computer Science (MFCS) , Palaiseau, France, 2025.\nRobert Braden, David Borman, and Craig Partridge. Computing the internet checksum. RFC 1071,\n1988. URL https://www.rfc-editor.org/rfc/rfc1071 .\nAdrian Bran, Mario Krenn, Philipp Kukura, and Yarin Gal. Chemcrow: Augmenting large-language\nmodels with chemistry tools. arXiv preprint arXiv:2304.05376 , 2023.\nCornelius Brand, Robert Ganian, and Mathis Rocton. New complexity-theoretic frontiers of\ntractability for neural network training. Advances in Neural Information Processing Systems ,\n36:56456\u201356468, 2023.\n47\n\n--- Page 48 ---\nKratsios, Zvigelsky, and Hart\nRoman Bresson, Johanne Cohen, Eyke H\u00a8 ullermeier, Christophe Labreuche, and Michele Sebag.\nNeural representation and learning of hierarchical 2-additive choquet integrals. In IJCAI-PRICAI-\n20-Twenty-Ninth International Joint Conference on Artificial Intelligence and Seventeenth Pacific\nRim International Conference on Artificial Intelligence , pages 1984\u20131991. International Joint\nConferences on Artificial Intelligence Organization, 2020.\nMichael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veli\u02c7 ckovi\u00b4 c. Geometric deep learning:\nGrids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478 , 2021.\nHans Buehler, Lukas Gonon, Josef Teichmann, and Ben Wood. Deep hedging. Quantitative Finance ,\n19(8):1271\u20131291, 2019.\nShengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George Em Karniadakis. Physics-\ninformed neural networks (pinns) for fluid mechanics: A review. Acta Mechanica Sinica , 37(12):\n1727\u20131738, 2021.\nChaoran Chen and Tanja Stadler. Genspectrum chat: data exploration in public health using large\nlanguage models. arXiv preprint arXiv:2305.13821 , 2023.\nSanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki\nKanda, Takuya Yoshioka, Xiong Xiao, et al. Wavlm: Large-scale self-supervised pre-training for\nfull stack speech processing. IEEE Journal of Selected Topics in Signal Processing , 16(6):1505\u2013\n1518, 2022.\nTianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural networks\nwith arbitrary activation functions and its application to dynamical systems. IEEE transactions\non neural networks , 6(4):911\u2013917, 1995.\nYifang Chen, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. The computational limits of\nstate-space models and mamba via the lens of circuit complexity. CoRR , abs/2412.06148, 2024.\nURL https://doi.org/10.48550/arXiv.2412.06148 .\nDavid Chiang. Transformers in uniform TC $\u02c60$.Transactions on Machine Learning Research ,\n2025. ISSN 2835-8856. URL https://openreview.net/forum?id=ZA7D4nQuQF .\nStephen Chung and Hava Siegelmann. Turing completeness of bounded-precision recurrent neural\nnetworks. Advances in neural information processing systems , 34:28431\u201328441, 2021.\nAlbert Cohen. Numerical analysis of wavelet methods , volume 32 of Studies in Mathematics and its\nApplications . North-Holland Publishing Co., Amsterdam, 2003. ISBN 0-444-51124-5.\nTrevor Cohn, Phil Blunsom, and Sharon Goldwater. Inducing tree-substitution grammars. The\nJournal of Machine Learning Research , 11:3053\u20133096, 2010.\nRama Cont and Purba Das. Quadratic variation and quadratic roughness. Bernoulli , 29(1):496\u2013522,\n2023.\nThomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to\nAlgorithms . MIT Press, 3 edition, 2009.\n48\n\n--- Page 49 ---\nSystematic Neural Network Representations of Algorithms\nAndrew Cropper, Sebastijan Duman\u02c7 ci\u00b4 c, Richard Evans, and Stephen H Muggleton. Inductive logic\nprogramming at 30. Machine Learning , 111(1):147\u2013172, 2022.\nGeorge Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,\nsignals and systems , 2(4):303\u2013314, 1989.\nIngrid Daubechies. Ten lectures on wavelets , volume 61 of CBMS-NSF Regional Conference Series\nin Applied Mathematics . Society for Industrial and Applied Mathematics (SIAM), Philadelphia,\nPA, 1992. ISBN 0-89871-274-2. doi: 10.1137/1.9781611970104. URL https://doi.org/10.\n1137/1.9781611970104 .\nArtur Back de Luca, George Giapitzakis, Shenghao Yang, Petar Veli\u02c7 ckovi\u00b4 c, and Kimon Foun-\ntoulakis. Positional attention: Expressivity and learnability of algorithmic computation. In\nForty-second International Conference on Machine Learning , 2025. URL https://openreview.\nnet/forum?id=0IJQD8zRXT .\nRonald A DeVore and Robert C Sharpley. Besov spaces on domains in R.Transactions of the\nAmerican Mathematical Society , 335(2):843\u2013864, 1993.\nAniket Didolkar, Anirudh Goyal, Nan Rosemary Ke, Siyuan Guo, Michal Valko, Timothy Lillicrap,\nDanilo Jimenez Rezende, Yoshua Bengio, Michael C Mozer, and Sanjeev Arora. Metacognitive\ncapabilities of LLMs: An exploration in mathematical problem solving. Advances in Neural\nInformation Processing Systems , 37:19783\u201319812, 2024.\nGiuseppe Alessio D\u2019Inverno, Monica Bianchini, Maria Lucia Sampoli, and Franco Scarselli. On the\napproximation capability of GNNs in node classification/regression tasks. Soft Computing , 28\n(13):8527\u20138547, 2024.\nAmitsour Egosi, Gilad Yehudai, and Ohad Shamir. Logarithmic width suffices for robust memo-\nrization, 2025. URL https://arxiv.org/abs/2502.11162 .\nDennis Elbr\u00a8 achter, Dmytro Perekrestenko, Philipp Grohs, and Helmut B\u00a8 olcskei. Deep neural net-\nwork approximation theory. IEEE Transactions on Information Theory , 67(5):2581\u20132623, 2021.\nYuhao Fan, Siwei Liu, Xiaozhi Yang, Deyu Ding, Zhixiong Yu, and Minlie Huang. FinQAPT:\nEmpowering Financial Decisions with End-to-End LLM-driven Question Answering Pipeline.\narXiv preprint arXiv:2410.13959 , 2024.\nNicola Rares Franco and Simone Brugiapaglia. A practical existence theorem for reduced order\nmodels based on convolutional autoencoders, 2024. URL https://arxiv.org/abs/2402.00435 .\nVincent Froese and Christoph Hertrich. Training neural networks is np-hard in fixed dimension.\nAdvances in Neural Information Processing Systems , 36:44039\u201344049, 2023.\nVincent Froese, Christoph Hertrich, and Rolf Niedermeier. The computational complexity of relu\nnetwork training parameterized by data dimensionality. Journal of Artificial Intelligence Research ,\n74:1775\u20131790, 2022.\nArvid Frydenlund. Language models, graph searching, and supervision adulteration: When more\nsupervision is less and how to make more more. arXiv preprint arXiv:2503.10542 , 2025.\n49\n",
  "project_dir": "artifacts/projects/enhanced_cs.NE_2508.18526v1_Quantifying_The_Limits_of_AI_Reasoning_Systematic",
  "communication_dir": "artifacts/projects/enhanced_cs.NE_2508.18526v1_Quantifying_The_Limits_of_AI_Reasoning_Systematic/.agent_comm",
  "assigned_at": "2025-08-31T21:01:48.738787",
  "status": "assigned"
}