{
  "agent_id": "coder2",
  "task_id": "task_6",
  "files": [
    {
      "name": "requirements.txt",
      "purpose": "Python dependencies",
      "priority": "high"
    },
    {
      "name": "preprocessing.py",
      "purpose": "Image preprocessing utilities",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.NE_2508.18526v1_Quantifying_The_Limits_of_AI_Reasoning_Systematic",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on cs.NE_2508.18526v1_Quantifying-The-Limits-of-AI-Reasoning-Systematic with content analysis. Detected project type: computer vision (confidence score: 8 matches).",
    "key_algorithms": [
      "Ceiling",
      "Resulting",
      "Machine",
      "Prompt",
      "Floor",
      "Representation",
      "Manifold",
      "Operator",
      "Learning",
      "Optimal"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "\n--- chunk_2.txt ---\nPDF: cs.NE_2508.18526v1_Quantifying-The-Limits-of-AI-Reasoning-Systematic.pdf\nChunk: 2/2\n==================================================\n\n--- Page 50 ---\nKratsios, Zvigelsky, and Hart\nKunihiko Fukushima. Visual feature extraction by a multilayered network of analog threshold\nelements. IEEE Transactions on Systems Science and Cybernetics , 5(4):322\u2013333, 1969.\nKen-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks.\nNeural networks , 2(3):183\u2013192, 1989.\nTakashi Furuya and Anastasis Kratsios. Simultaneously Solving FBSDEs with Neural Operators\nof Logarithmic Depth, Constant Width, and Sub-Linear Rank, 2024. URL https://arxiv.org/\nabs/2410.14788 .\nTakashi Furuya, Koichi Taniguchi, and Satoshi Okuda. Quantitative approximation for neural\noperators in nonlinear parabolic equations, 2024. URL https://arxiv.org/abs/2410.02151 .\nFernando Gama, Antonio G Marques, Geert Leus, and Alejandro Ribeiro. Convolutional neural\nnetwork architectures for signals supported on graphs. IEEE Transactions on Signal Processing ,\n67(4):1034\u20131049, 2018.\nRobert Ganian, Mathis Rocton, and Simon Wietheger. Training one-dimensional graph neural\nnetworks is np-hard. In The Thirteenth International Conference on Learning Representations ,\n2025.\nLukas Gonon. Random feature neural networks learn Black-Scholes type PDEs without curse of\ndimensionality. Journal of Machine Learning Research , 24(189):1\u201351, 2023.\nR\u00b4 emi Gribonval, Gitta Kutyniok, Morten Nielsen, and Felix Voigtlaender. Approximation spaces\nof deep neural networks. Constructive approximation , 55(1):259\u2013367, 2022.\nIngo G\u00a8 uhring and Mones Raslan. Approximation rates for neural networks with encodable weights\nin smoothness spaces. Neural Networks , 134:107\u2013130, 2021.\nRuiyang Hong and Anastasis Kratsios. Bridging the gap between approximation and learning via\noptimal approximation by ReLU MLPs of maximal regularity. arXiv preprint arXiv:2409.12335 ,\n2024.\nKurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-\nversal approximators. Neural networks , 2(5):359\u2013366, 1989.\nBlanka Hovart, Anastasis Kratsios, Yannick Limmer, and Xuwei Yang. Deep Kalman Filters Can\nFilter. arXiv preprint arXiv:2310.19603 , 2023.\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,\nand Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked\nprediction of hidden units. IEEE/ACM transactions on audio, speech, and language processing ,\n29:3451\u20133460, 2021.\nItay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized\nneural networks: Training neural networks with low precision weights and activations. Journal\nof Machine Learning Research , 18(187):1\u201330, 2018.\n50\n\n--- Page 51 ---\nSystematic Neural Network Representations of Algorithms\nMartin Hutzenthaler, Arnulf Jentzen, Thomas Kruse, Tuan Anh Nguyen, and Philippe von\nWurstemberger. Overcoming the curse of dimensionality in the numerical approximation of semi-\nlinear parabolic partial differential equations. Proceedings of the Royal Society A , 476(2244):\n20190630, 2020a.\nMartin Hutzenthaler, Arnulf Jentzen, and Philippe von Wurstemberger. Overcoming the curse of\ndimensionality in the approximative pricing of financial derivatives with default risks. Electron.\nJ. Probab. , 25:Paper No. 101, 73, 2020b. ISSN 1083-6489. doi: 10.1214/20-ejp423. URL https:\n//doi.org/10.1214/20-ejp423 .\nMartin Hutzenthaler, Arnulf Jentzen, Thomas Kruse, and Tuan Anh Nguyen. Overcoming the curse\nof dimensionality in the numerical approximation of backward stochastic differential equations.\nJ. Numer. Math. , 31(1):1\u201328, 2023. ISSN 1570-2820,1569-3953. doi: 10.1515/jnma-2021-0111.\nURL https://doi.org/10.1515/jnma-2021-0111 .\nBenoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,\nHartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for\nefficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 2704\u20132713, 2018.\nStefanie Jegelka. Theory of graph neural networks: Representation and learning. In The Interna-\ntional Congress of Mathematicians , pages 1\u201323, 2022.\nStasys Jukna. Boolean function complexity , volume 27 of Algorithms and Combinatorics . Springer,\nHeidelberg, 2012a. ISBN 978-3-642-24507-7. doi: 10.1007/978-3-642-24508-4. URL https:\n//doi.org/10.1007/978-3-642-24508-4 . Advances and frontiers.\nStasys Jukna. Boolean function complexity , volume 27 of Algorithms and Combinatorics . Springer,\nHeidelberg, 2012b. ISBN 978-3-642-24507-7. doi: 10.1007/978-3-642-24508-4. URL https:\n//doi.org/10.1007/978-3-642-24508-4 . Advances and frontiers.\nStasys Jukna. Tropical circuit complexity\u2014limits of pure dynamic programming . SpringerBriefs in\nMathematics. Springer, Cham, 2023. ISBN 978-3-031-42353-6; 978-3-031-42354-3. doi: 10.1007/\n978-3-031-42354-3. URL https://doi.org/10.1007/978-3-031-42354-3 .\nJean-Pierre Kahane. Sur le th\u00b4 eor` eme de superposition de kolmogorov. Journal of Approxima-\ntion Theory , 13(3):229\u2013234, 1975. ISSN 0021-9045. doi: https://doi.org/10.1016/0021-9045(75)\n90035-0. URL https://www.sciencedirect.com/science/article/pii/0021904575900350 .\nMarek Karpinski and Angus Macintyre. Polynomial bounds for vc dimension of sigmoidal and\ngeneral pfaffian neural networks. Journal of Computer and System Sciences , 54(1):169\u2013176, 1997.\nSteve Klabnik and Carol Nichols. The Rust programming language . No Starch Press, 2023.\nDonald E. Knuth. The Art of Computer Programming, Volume 2: Seminumerical Algorithms .\nAddison-Wesley, 3 edition, 1997.\nMichael Kohler and Sophie Langer. On the rate of convergence of fully connected deep neural\nnetwork regression estimates. Ann. Statist. , 49(4):2231\u20132249, 2021. ISSN 0090-5364,2168-8966.\ndoi: 10.1214/20-aos2034. URL https://doi.org/10.1214/20-aos2034 .\n51\n\n--- Page 52 ---\nKratsios, Zvigelsky, and Hart\nA. N. Kolmogorov. On the representation of continuous functions of several variables by superpo-\nsitions of continuous functions of a smaller number of variables. Dokl. Akad. Nauk SSSR (N.S.) ,\n108:179\u2013182, 1956.\nA. N. Kolmogorov. On the representation of continuous functions of several variables by superposi-\ntions of continuous functions of a smaller number of variables. Amer. Math. Soc. Transl. (2) , 17:\n369\u2013373, 1961. doi: 10.1090/trans2/017/12. URL https://doi.org/10.1090/trans2/017/12 .\nAnastasis Kratsios and Takashi Furuya. Is In-Context Universality Enough? MLPs are Also Uni-\nversal In-Context. arXiv preprint arXiv:2502.03327 , 2025.\nAnastasis Kratsios and L\u00b4 eonie Papon. Universal approximation theorems for differentiable geometric\ndeep learning. Journal of Machine Learning Research , 23(196):1\u201373, 2022.\nAnastasis Kratsios and Behnoosh Zamanlooy. Do ReLU networks have an edge when approximating\ncompactly-supported functions? Transactions on Machine Learning Research , 2022. ISSN 2835-\n8856. URL https://openreview.net/forum?id=sNxNi54B8b .\nAnastasis Kratsios, Behnoosh Zamanlooy, Tianlin Liu, and Ivan Dokmani\u00b4 c. Universal approxima-\ntion under constraints is possible with transformers. In International Conference on Learning\nRepresentations , 2022. URL https://openreview.net/forum?id=JGO8CvG5S9 .\nAnastasis Kratsios, A Martina Neuman, and Gudmund Pammer. Tighter learning guarantees on\ndigital computers via concentration of measure on finite spaces. ArXiV , 2024.\nAnastasis Kratsios, Ariel Neufeld, and Philipp Schmocker. Generative neural operators of\nlog-complexity can simultaneously solve infinitely many convex programs. arXiv preprint\narXiv:2508.14995 , 2025a.\nAnastasis Kratsios, Philip Schmoker, and Ariel Neufeld. Exponential approximation rates by neural\noperators solutions to convex splitting problens. ArXiV , 2025b.\nGustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep neural net-\nworks without residuals. In International Conference on Learning Representations , 2017.\nPierre L\u2019Ecuyer. Random number generation. In Handbook of Computational Statistics , pages\n35\u201371. Springer, 2012.\nAndrei Leman and Boris Weisfeiler. A reduction of a graph to a canonical form and an algebra\narising during this reduction. Nauchno-Technicheskaya Informatsiya , 2(9):12\u201316, 1968.\nJierui Li, Lei Wang, Jipeng Zhang, Yan Wang, Bingtian Dai, and Dongxiang Zhang. Modeling intra-\nrelation in math word problems with different functional multi-head attentions. In Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics , pages 6013\u20136022.\nAssociation for Computational Linguistics, 2019.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image\npre-training with frozen image encoders and large language models. In International conference\non machine learning , pages 19730\u201319742. PMLR, 2023a.\n52\n\n--- Page 53 ---\nSystematic Neural Network Representations of Algorithms\nShucheng Li, Lingfei Wu, Shiwei Feng, Fangli Xu, Fengyuan Xu, and Sheng Zhong. Graph-to-tree\nneural networks for learning structured input-output translation with applications to semantic\nparsing and math word problem. arXiv preprint arXiv:2004.13781 , 2020.\nYiming Li, Jianfu Li, Jianping He, and Cui Tao. Ae-gpt: using large language models to extract\nadverse events from surveillance reports-a use case with influenza vaccine adverse events. Plos\none, 19(3):e0300919, 2024a.\nZhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of thought empowers transformers to\nsolve inherently serial problems. In The Twelfth International Conference on Learning Represen-\ntations , 2024b. URL https://openreview.net/forum?id=3EWTEy9MTM .\nZongyi Li, Daniel Zhengyu Huang, Burigede Liu, and Anima Anandkumar. Fourier neural operator\nwith learned deformations for pdes on general geometries. Journal of Machine Learning Research ,\n24(388):1\u201326, 2023b.\nBingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers\nlearn shortcuts to automata. In The Eleventh International Conference on Learning Representa-\ntions, 2023. URL https://openreview.net/forum?id=De4FYqjFueZ .\nAntonio Lloris Ruiz, Encarnaci\u00b4 on Castillo Morales, Luis Parrilla Roure, and Antonio Garc\u00b4 \u0131a R\u00b4 \u0131os.\nAlgebraic circuits , volume 66 of Intelligent Systems Reference Library . Springer, Heidelberg,\n2014. ISBN 978-3-642-54648-8; 978-3-642-54649-5. doi: 10.1007/978-3-642-54649-5. URL https:\n//doi.org/10.1007/978-3-642-54649-5 .\nAntonio Lloris Ruiz, Encarnaci\u00b4 on Castillo Morales, Luis Parrilla Roure, Antonio Garc\u00b4 \u0131a R\u00b4 \u0131os, and\nMar\u00b4 \u0131a Jos\u00b4 e Lloris Meseguer. Arithmetic and algebraic circuits , volume 201 of Intelligent Systems\nReference Library . Springer, Cham, 2021. ISBN 978-3-030-67266-9; 978-3-030-67265-2. doi:\n10.1007/978-3-030-67266-9. URL https://doi.org/10.1007/978-3-030-67266-9 .\nGeorge G. Lorentz, Manfred v. Golitschek, and Yuly Makovoz. Constructive approximation , volume\n304 of Grundlehren der mathematischen Wissenschaften [Fundamental Principles of Mathematical\nSciences] . Springer-Verlag, Berlin, 1996a. ISBN 3-540-57028-4. doi: 10.1007/978-3-642-60932-9.\nURL https://doi.org/10.1007/978-3-642-60932-9 . Advanced problems.\nGeorge G. Lorentz, Manfred v. Golitschek, and Yuly Makovoz. Constructive approximation , vol-\nume 304 of Grundlehren der mathematischen Wissenschaften . Springer-Verlag, Berlin, 1996b.\nISBN 3-540-57028-4. doi: 10.1007/978-3-642-60932-9. URL https://doi.org/10.1007/\n978-3-642-60932-9 . Advanced problems.\nSA Lozhkin. Tighter bounds on the complexity of control systems from some classes. mat. Voprosy\nKibernetiki , 6:189\u2013214, 1996.\nJianfeng Lu, Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network approximation for\nsmooth functions. SIAM Journal on Mathematical Analysis , 53(5):5465\u20135506, 2021a.\nLu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning\nnonlinear operators via deeponet based on the universal approximation theorem of operators.\nNature machine intelligence , 3(3):218\u2013229, 2021b.\n53\n\n--- Page 54 ---\nKratsios, Zvigelsky, and Hart\nPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter\nClark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured\nmathematical reasoning. In The Eleventh International Conference on Learning Representations ,\n2023. URL https://openreview.net/forum?id=DHyHRBwJUTN .\nRenqian Luo, Xian Sun, Yingce Xia, Tao Qin, Sheng Zhang, and Tie-Yan Liu. Biogpt: Gen-\nerative pre-trained transformer for biomedical text generation and mining. arXiv preprint\narXiv:2210.10341 , 2022.\nYao Ma, Xiaorui Liu, Tong Zhao, Yozen Liu, Jiliang Tang, and Neil Shah. A unified view on\ngraph neural networks as graph signal denoising. In Proceedings of the 30th ACM international\nconference on information & knowledge management , pages 1202\u20131211, 2021.\nCarlo Marcati and Christoph Schwab. Exponential convergence of deep operator networks for elliptic\npartial differential equations. SIAM Journal on Numerical Analysis , 61(3):1513\u20131545, 2023. doi:\n10.1137/21M1465718.\nCarlo Marcati, Joost AA Opschoor, Philipp C Petersen, and Christoph Schwab. Exponential relu\nneural network approximation rates for point and edge singularities. Foundations of Computa-\ntional Mathematics , 23(3):1043\u20131127, 2023.\nDavid Marker. Model theory , volume 217 of Graduate Texts in Mathematics . Springer-Verlag, New\nYork, 2002. ISBN 0-387-98760-6. An introduction.\nLeland McInnes, John Healy, Nathaniel Saul, and Lukas Gro\u00dfberger. Umap: Uniform manifold\napproximation and projection. Journal of Open Source Software , 3(29):861, 2018.\nAlfred J. Menezes, Paul C. van Oorschot, and Scott A. Vanstone. Handbook of Applied Cryptography .\nCRC Press, 1996.\nWilliam Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision trans-\nformers. Transactions of the Association for Computational Linguistics , 11:531\u2013545, 2023.\nHrushikesh N Mhaskar and Tomaso Poggio. Deep vs. shallow networks: An approximation theory\nperspective. Analysis and Applications , 14(06):829\u2013848, 2016.\nJos\u00b4 e L Monta\u02dc na and Luis M Pardo. On the Vapnik-Chervonenkis dimension of computer pro-\ngrams which use transcendental elementary operations. Annals of Mathematics and Artificial\nIntelligence , 56:371\u2013388, 2009.\nJason Morton and Jacob Turner. Generalized counting constraint satisfaction problems with deter-\nminantal circuits. Linear Algebra and its Applications , 466:357\u2013381, 2015.\nDavid E. Muller. Complexity in electronic switching circuits. IRE Transactions on Electronic\nComputers , EC-8(1):15\u201319, 1959.\nAriel Neufeld and Philipp Schmocker. Universal approximation results for neural networks with\nnon-polynomial activation function over non-compact domains. arXiv preprint arXiv:2410.14759 ,\n2024.\n54\n\n--- Page 55 ---\nSystematic Neural Network Representations of Algorithms\nA Martina Neuman, Andres Felipe Lerma Pineda, Jason J Bramburger, and Simone Brugiapaglia.\nReconstruction of frequency-localized functions from pointwise samples via least squares and deep\nlearning. arXiv preprint arXiv:2502.09794 , 2025.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David\nBieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and\nAugustus Odena. Show your work: Scratchpads for intermediate computation with language\nmodels, 2022. URL https://openreview.net/forum?id=iedYJm92o0a .\nIan Parberry. Circuit complexity and neural networks . MIT press, 1994.\nDavid A. Patterson and John L. Hennessy. Computer Organization and Design: The Hardware/-\nSoftware Interface . Morgan Kaufmann, 6 edition, 2020.\nPhilipp Petersen and Felix Voigtlaender. Optimal approximation of piecewise smooth functions\nusing deep ReLU neural networks. Neural Networks , 108:296\u2013330, 2018.\nPhilipp Petersen and Felix Voigtlaender. Equivalence of approximation by convolutional neural\nnetworks and fully-connected networks. Proceedings of the American Mathematical Society , 148\n(4):1567\u20131581, 2020.\nPhilipp Petersen and Jakob Zech. Mathematical theory of deep learning. arXiv preprint\narXiv:2407.18384 , 2024.\nNicholas Pippenger and Michael J. Fischer. Relations among complexity measures. J. Assoc.\nComput. Mach. , 26(2):361\u2013381, 1979. ISSN 0004-5411,1557-735X. doi: 10.1145/322123.322138.\nURL https://doi.org/10.1145/322123.322138 .\nJorge P\u00b4 erez, Javier Marinkovi\u00b4 c, and Pablo Barcel\u00b4 o. On the turing completeness of modern neural\nnetwork architectures. In International Conference on Learning Representations , 2019. URL\nhttps://openreview.net/forum?id=HyGBdo0qFm .\nShi Qiu, Shaoyang Guo, Zhuo-Yang Song, Yunbo Sun, Zeyu Cai, Jiashen Wei, Tianyu Luo, Yixuan\nYin, Haoxu Zhang, Yi Hu, et al. Phybench: Holistic evaluation of physical perception and\nreasoning in large language models. arXiv preprint arXiv:2504.16074 , 2025.\nMichael O. Rabin. Fingerprinting by random polynomials. Technical Report TR-15-81, Harvard\nUniversity, 1981.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research , 21(140):1\u201367, 2020.\nRyan Riegel, Alexander Gray, Francois Luus, Naweed Khan, Ndivhuwo Makondo, Ismail Yunus\nAkhalwaya, Haifeng Qian, Ronald Fagin, Francisco Barahona, Udit Sharma, et al. Logical neural\nnetworks. arXiv preprint arXiv:2006.13155 , 2020.\nErwin Riegler, Alex B\u00a8 uhler, Yang Pan, and Helmut B\u00a8 olcskei. Generating rectifiable measures\nthrough neural networks. arXiv preprint arXiv:2412.05109 , 2024.\n55\n\n--- Page 56 ---\nKratsios, Zvigelsky, and Hart\nClayton Sanford, Bahare Fatemi, Ethan Hall, Anton Tsitsulin, Mehran Kazemi, Jonathan Halcrow,\nBryan Perozzi, and Vahab Mirrokni. Understanding transformer reasoning capabilities via graph\nalgorithms. Advances in Neural Information Processing Systems , 37:78320\u201378370, 2024a.\nClayton Sanford, Daniel Hsu, and Matus Telgarsky. Transformers, parallel computation, and loga-\nrithmic depth. In Proceedings of the 41st International Conference on Machine Learning , pages\n43276\u201343327, 2024b.\nRaeid Saqur, Anastasis Kratsios, Florian Krach, Yannick Limmer, Blanka Horvath, and Frank\nRudzicz. Filtered not mixed: Filtering-based online gating for mixture of large language models.\nInThe Thirteenth International Conference on Learning Representations , 2025. URL https:\n//openreview.net/forum?id=ecIvumCyAj .\nRyoma Sato. A survey on the expressive power of graph neural networks. arXiv preprint\narXiv:2003.04078 , 2020.\nJohannes Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU\nactivation function. Ann. Statist. , 48(4):1875\u20131897, 2020. ISSN 0090-5364,2168-8966. doi:\n10.1214/19-AOS1875. URL https://doi.org/10.1214/19-AOS1875 .\nDale Schuurmans, Hanjun Dai, and Francesco Zanini. Autoregressive large language models are\ncomputationally universal. arXiv preprint arXiv:2410.03170 , 2024.\nChristoph Schwab and Jakob Zech. Deep learning in high dimension: Neural network expression\nrates for generalized polynomial chaos expansions in uq. Analysis and Applications , 17(01):19\u201355,\n2019.\nChristoph Schwab and Jakob Zech. Deep learning in high dimension: neural network expression\nrates for analytic functions. SIAM/ASA Journal on Uncertainty Quantification , 11(1):199\u2013234,\n2023.\nZuowei Shen, Haizhao Yang, and Shijun Zhang. Optimal approximation rate of ReLU networks in\nterms of width and depth. J. Math. Pures Appl. (9) , 157:101\u2013135, 2022. ISSN 0021-7824,1776-\n3371. doi: 10.1016/j.matpur.2021.07.009. URL https://doi.org/10.1016/j.matpur.2021.07.\n009.\nVictor Shoup. A Computational Introduction to Number Theory and Algebra . Cambridge University\nPress, 2 edition, 2009.\nJonathan W Siegel and Jinchao Xu. Sharp bounds on the approximation rates, metric entropy, and\nn-widths of shallow neural networks. Foundations of Computational Mathematics , 24(2):481\u2013537,\n2024.\nHava T Siegelmann. Neural networks and analog computation: beyond the Turing limit . Springer\nScience & Business Media, 2012.\nManjot Singh, Adalbert Fono, and Gitta Kutyniok. Expressivity of spiking neural networks through\nthe spike response model. In UniReps: the First Workshop on Unifying Representations in Neural\nModels , 2023. URL https://openreview.net/forum?id=Qms9kWnbpP .\n56\n\n--- Page 57 ---\nSystematic Neural Network Representations of Algorithms\nWilliam Stallings. Cryptography and Network Security: Principles and Practice . Pearson, 8 edition,\n2020.\nTaiji Suzuki. Adaptivity of deep reLU network for learning in besov and mixed smooth besov spaces:\noptimal rate and curse of dimensionality. In International Conference on Learning Representa-\ntions, 2019. URL https://openreview.net/forum?id=H1ebTsActm .\nMatus Telgarsky. Representation benefits of deep feedforward networks. arXiv preprint\narXiv:1509.08101 , 2015.\nJoshua B Tenenbaum, Vin de Silva, and John C Langford. A global geometric framework for\nnonlinear dimensionality reduction. science , 290(5500):2319\u20132323, 2000.\nHans Triebel. Function spaces and wavelets on domains , volume 7 of EMS Tracts in Mathematics .\nEuropean Mathematical Society (EMS), Z\u00a8 urich, 2008. ISBN 978-3-03719-019-7. doi: 10.4171/019.\nURL https://doi.org/10.4171/019 .\nTeun DH van Nuland. Noncompact uniform universal approximation. Neural Networks , 173:106181,\n2024.\nGal Vardi, Gilad Yehudai, and Ohad Shamir. On the optimal memorization power of ReLU neural\nnetworks. arXiv preprint arXiv:2110.03187 , 2021.\nPetar Veli\u02c7 ckovi\u00b4 c. Everything is connected: Graph neural networks. Current Opinion in Structural\nBiology , 79:102538, 2023.\nFelix Voigtlaender. The universal approximation theorem for complex-valued neural networks.\nApplied and computational harmonic analysis , 64:33\u201361, 2023.\nHeribert Vollmer. Introduction to circuit complexity: a uniform approach . Springer Science &\nBusiness Media, 1999.\nBenjie Wang, Denis Deratani Mau\u00b4 a, Guy Van den Broeck, and YooJung Choi. A compositional\natlas for algebraic circuits. arXiv preprint arXiv:2412.05481 , 2024.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nneural information processing systems , 35:24824\u201324837, 2022.\nKlaus Weihrauch. Computable analysis . Texts in Theoretical Computer Science. An EATCS Series.\nSpringer-Verlag, Berlin, 2000. ISBN 3-540-66817-9. doi: 10.1007/978-3-642-56999-9. URL https:\n//doi.org/10.1007/978-3-642-56999-9 . An introduction.\nPrzemyslaw Wojtaszczyk. A mathematical introduction to wavelets , volume 37. Cambridge Univer-\nsity Press, 1997.\nZhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Aky\u00a8 urek, Boyuan Chen, Bailin Wang, Najoung Kim,\nJacob Andreas, and Yoon Kim. Reasoning or reciting? exploring the capabilities and limitations\nof language models through counterfactual tasks. CoRR , abs/2307.02477, 2023. URL https:\n//doi.org/10.48550/arXiv.2307.02477 .\n57\n\n--- Page 58 ---\nKratsios, Zvigelsky, and Hart\nAdrian Wurm. Complexity of reachability problems in neural networks. In International Conference\non Reachability Problems , pages 15\u201327. Springer, 2023.\nAdrian Wurm. Robustness verification in neural networks. In International Conference on the\nIntegration of Constraint Programming, Artificial Intelligence, and Operations Research , pages\n263\u2013278. Springer, 2024.\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\nnetworks? In International Conference on Learning Representations , 2019. URL https://\nopenreview.net/forum?id=ryGs6iA5Km .\nXin Xu, Qiyun Xu, Tong Xiao, Tianhao Chen, Yuchen Yan, Jiaxin Zhang, Shizhe Diao, Can Yang,\nand Yang Wang. Ugphysics: A comprehensive benchmark for undergraduate physics reasoning\nwith large language models. arXiv preprint arXiv:2502.00334 , 2025.\nBitao Yang, Zichuan Chen, Zhichao Zhao, Xiao Lin, Yiran Liang, Yankai Liu, Zihao Zhang, Yimeng\nWang, Junxiao Yan, Yifei Zhang, et al. Fingpt: Democratizing internet-scale data for financial\nlarge language models. arXiv preprint arXiv:2307.10485 , 2023a.\nBitao Yang, Xiao Lin, Zhichao Zhao, Zihao Zhang, Yiran Liang, Zichuan Chen, Yimeng Wang,\nYifei Zhang, Yankai Liu, Bill Yuchen Lin, et al. Instruct-fingpt: Financial sentiment analysis by\ninstruction tuning of general-purpose large language models. arXiv preprint arXiv:2306.12659 ,\n2023b.\nYahong Yang and Yulong Lu. Near-optimal deep neural network approximation for korobov func-\ntions with respect to lp and h1 norms. Neural Networks , 180:106702, 2024.\nDmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural networks , 94:\n103\u2013114, 2017.\nDmitry Yarotsky. Optimal approximation of continuous functions by very deep ReLU networks. In\nConference on learning theory , pages 639\u2013649. PMLR, 2018.\nDmitry Yarotsky. Elementary superexpressive activations. In Marina Meila and Tong Zhang,\neditors, Proceedings of the 38th International Conference on Machine Learning , volume 139 of\nProceedings of Machine Learning Research , pages 11932\u201311940. PMLR, 18\u201324 Jul 2021. URL\nhttps://proceedings.mlr.press/v139/yarotsky21a.html .\nDmitry Yarotsky. Universal approximations of invariant maps by neural networks. Constructive\nApproximation , 55(1):407\u2013474, 2022.\nZheng Zeng, Rodney M Goodman, and Padhraic Smyth. Discrete recurrent neural networks as push-\ndown automata. In International symposium on nonlinear theory and its applications , volume 3,\npages 1033\u20131038. Citeseer, 1993.\nShijun Zhang, Zuowei Shen, and Haizhao Yang. Deep network approximation: Achieving arbitrary\naccuracy with fixed number of neurons. Journal of Machine Learning Research , 23(276):1\u201360,\n2022.\nShijun Zhang, Jianfeng Lu, and Hongkai Zhao. Deep network approximation: Beyond RELU to\ndiverse activation functions. Journal of Machine Learning Research , 25(35):1\u201339, 2024.\n58\n\n--- Page 59 ---\nSystematic Neural Network Representations of Algorithms\nXingcheng Zhang, Zhizhong Li, Chen Change Loy, and Dahua Lin. Polynet: A pursuit of structural\ndiversity in very deep networks. In Proceedings of the IEEE conference on computer vision and\npattern recognition , pages 718\u2013726, 2017.\nXinyu Zhang, Yuxuan Dong, Yanrui Wu, Jiaxing Huang, Chengyou Jia, Basura Fernando,\nMike Zheng Shou, Lingling Zhang, and Jun Liu. Physreason: A comprehensive benchmark\ntowards physics-based reasoning. arXiv preprint arXiv:2502.12054 , 2025.\nYani Zhang and Helmut B\u00a8 olcskei. Extracting formulae in many-valued logic from deep neural\nnetworks. arXiv preprint arXiv:2401.12113 , 2024.\nYujia Zhang, Zhiwei Zhu, Ming Zhang, Shujie Zhang, Jie Liu, and Xiaoyan Zhu. Biomedgpt: Open\nmultimodal generative pre-trained transformer for biomedicine. arXiv preprint arXiv:2308.09442 ,\n2023.\n59\n",
  "project_dir": "artifacts/projects/enhanced_cs.NE_2508.18526v1_Quantifying_The_Limits_of_AI_Reasoning_Systematic",
  "communication_dir": "artifacts/projects/enhanced_cs.NE_2508.18526v1_Quantifying_The_Limits_of_AI_Reasoning_Systematic/.agent_comm",
  "assigned_at": "2025-08-31T21:03:49.872658",
  "status": "assigned"
}